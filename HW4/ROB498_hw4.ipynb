{"cells": [{"cell_type": "markdown", "metadata": {"id": "L5onTnjdhwp-"}, "source": ["# ROB 498: Robot Learning for Planning and Control\n", "# Assignment 4: Learning Dynamics with Gaussian Processes"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "vygmdisshwqI"}, "source": ["## Colab Setup"], "outputs": []}, {"cell_type": "code", "execution_count": 41, "metadata": {"id": "4XMaQBHChwqJ"}, "outputs": [], "source": ["# TODO: Fill in the Google Drive path where you uploaded the assignment\n", "# Example: If you create a ROB498 folder and put all the files under HW3 folder, then 'ROB498/HW4'\n", "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ROB498/HW4'"]}, {"cell_type": "markdown", "metadata": {"id": "R6kkuGdhhwqN"}, "source": ["### Setup Code "], "outputs": []}, {"cell_type": "code", "execution_count": 42, "metadata": {"id": "PTvxidW-hwqO", "outputId": "c992c638-58e3-4326-e778-16e12acc5e95"}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "code", "execution_count": 43, "metadata": {"id": "9QLYIIyVhwqQ", "outputId": "e3d28735-8513-44f6-94ca-af7102f9d177"}, "outputs": [], "source": ["from google.colab import drive\n", "\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"id": "cm4KAZmQhwqR", "outputId": "f152e757-a0b7-4195-cea9-a8f526a549e3"}, "outputs": [], "source": ["import os\n", "import sys\n", "\n", "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n", "\n", "files = os.listdir(GOOGLE_DRIVE_PATH)\n", "expected_files = ['ROB498_hw4.ipynb', \n", "                  'gpytorch_intro.py', \n", "                  'learning_uncertain_dynamics.py', \n", "                  'mppi.py', \n", "                  'utils.py', \n", "                  'visualizers.py', \n", "                  'panda_pushing_env.py',\n", "                  'q1_train_data.npz',\n", "                  'pushing_training_data.npy', \n", "                  'pushing_validation_data.npy', \n", "                  'visualization_data.npz', \n", "                 ]\n", "\n", "sys.path.append(GOOGLE_DRIVE_PATH)\n", "\n", "# Verify that there are all the expected files in the directory\n", "all_found = True\n", "for expected_file in expected_files:\n", "  if expected_file not in files:\n", "    print(f'Required file {expected_file} not found!')\n", "    all_found = False\n", "if all_found:\n", "  print('All required files are found :)')"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"id": "NU0e6yBXhwqT", "outputId": "97ef7805-cd4b-46a7-fc04-401f701102f1"}, "outputs": [], "source": ["# Install missing required packages \n", "# Unfortunately Colab does not have pybullet package by default, so we will have to install it every time that the notebook kernel is restarted.\n", "# Install pybullet -- For simulation purposes\n", "!pip install pybullet\n", "# Install numpngw -- For visualization purposes\n", "!pip install numpngw\n", "# Install GPyTorch\n", "!pip install "]}, {"cell_type": "code", "execution_count": 15, "metadata": {"id": "_mVbMxechwqU"}, "outputs": [], "source": ["import torch\n", "import os\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "import numpy as np\n", "from torch.utils.data import Dataset, DataLoader\n", "import matplotlib.pyplot as plt\n", "from numpngw import write_apng\n", "from IPython.display import Image\n", "from tqdm.notebook import tqdm\n", "import gpytorch"]}, {"cell_type": "markdown", "metadata": {"id": "GzJpjfzJhwqV"}, "source": ["## Assignment Introduction"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "cI1b3CEJhwqW"}, "source": ["Due 3/29 at 11:59pm\n", "\n", "**Rules**:\n", "\n", "1. All homework must be done individually, but you are encouraged to post questions on Piazza\n", "\n", "2. No late homework will be accepted (unless you use your late-day tokens)\n", "\n", "3. Submit your code on [autograder.io](http://autograder.io/)\n", "\n", "4. Remember that copying-and-pasting code from other sources is not allowed\n", "\n", "5. The use of additional package imports beyound the packages we provide is not allowed. The autograder will not grade your code if you use additional packages.\n", "\n", "**Instructions**\n", "- Each problem will give you a file with some template code, and you need to fill in the\n", "rest.\n", "- We use the autograder, so if you\u2019re ever wondering \u201cwill I get full points for\n", "this?\u201d just upload your code in the autograder to check. There is no limit to how\n", "many times you can upload to autograder.\n", "- The autograder may test your problem with multiple different inputs to make sure it is correct.\n", "- The autograder will only show you if you got it right/wrong, so if you don\u2019t get full points, try to test with some other inputs."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "koRG2QrThwqX"}, "source": ["# 1. Introduction to GPyTorch (20 points)\n", "\n", "In this section you will use GPyTorch to fit a Gaussian Process (GP) to the following 1D function\n", "\n", "\n", "$$ y = \\sin (\\pi x) + 1.73 x $$\n", "\n", "Where the data is corrupted by noise. We will explore how different choices of kernels affect the resulting GP fit. \n", "\n"], "outputs": []}, {"cell_type": "code", "execution_count": 16, "metadata": {"id": "XXd425bNhwqY", "outputId": "b165d3d6-a6ea-42f1-f0ad-afddae33aa60"}, "outputs": [], "source": ["from gpytorch_intro import *\n", "\n", "# First load the training data\n", "train_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'q1_train_data.npz' ), allow_pickle=True)\n", "\n", "train_x = torch.from_numpy(data['x'])\n", "train_y = torch.from_numpy(data['y'])\n", "\n", "test_x = torch.linspace(-3, 3, 100) \n", "\n", "plt.scatter(train_x, train_y)\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"id": "LpcOP2NGhwqZ"}, "source": ["## 1.1 Training a GP with an RBF kernel (10 points)\n", "\n", "One of the most common kernels used in GP regression is the scaled Radial Basis Function (RBF) kernel, which is shown in the lecture slides. \n", "$$ k(x, x') = \\sigma ^2 e^{-\\frac{1}{2l^2} (x-x')^T(x-x')}$$\n", "The GPytorch RBF kernel is\n", "$$ k(x, x') = e^{-\\frac{1}{2l^2} (x-x')^T(x-x')}$$\n", "To incorporate $\\sigma^2$ you must compose the RBF kernel with a Scale Kernel. These kernels are `gpytorch.kernels.RBFKernel` and `gpytorch.kernels.ScaleKernel` respectively. \n", "\n", "### 1.1 a) Creating a GP with an RBF kernel (5 points)\n", "\n", "- Complete the function `RBF_GP.__init__` by defining `self.mean_module` and `self.covar_module`\n", "- You should use a zero-mean function\n", "- You should use the RBF kernel with the scale parameter"], "outputs": []}, {"cell_type": "code", "execution_count": 17, "metadata": {"id": "q29Ts51Xhwqa", "outputId": "403a4aca-3909-47f1-bac4-da8b85833426"}, "outputs": [], "source": ["# Define the RBF GP with a Gaussian likelihood\n", "rbf_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n", "rbf_model = RBF_GP(train_x, train_y, rbf_likelihood)\n", "\n", "\n", "print('Kernel function is correctly defined: ')\n", "\n", "print(\n", "    isinstance(rbf_model.covar_module, gpytorch.kernels.ScaleKernel) and \n", "    isinstance(rbf_model.covar_module.base_kernel, gpytorch.kernels.RBFKernel)\n", ")\n", "\n", "print('')\n", "print('Mean function is correctly defined: ')\n", "print(isinstance(rbf_model.mean_module, gpytorch.means.ZeroMean))\n"]}, {"cell_type": "markdown", "metadata": {"id": "aHyiswTjhwqa"}, "source": ["### 1.1 b) Optimizing GP hyperparameters (5 points)\n", "\n", "When we initialize a GP with GPytorch it will initialize with some set of hyperparameters for the Kernel and the likelihood. Let's see what happens when we use the initial hyperparameters"], "outputs": []}, {"cell_type": "code", "execution_count": 18, "metadata": {"id": "8DQXLupghwqa", "outputId": "e4276c09-3bca-40f9-b93c-abb61c640245"}, "outputs": [], "source": ["# Define the RBF kernel GP model\n", "rbf_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n", "rbf_model = RBF_GP(train_x, train_y, rbf_likelihood)\n", "\n", "# Plot the resulting fit GP\n", "plot_gp_predictions(rbf_model, rbf_likelihood, train_x, train_y, test_x, title='RBF kernel')"]}, {"cell_type": "markdown", "metadata": {"id": "lc6WYv67hwqb"}, "source": ["This fit is OK, but it is treating a lot of the variation in the data as noise. We can improve the fit by optimizing the GP hyperparameters. \n", "\n", "- Complete the function `train_gp_hyperparams` in `gpytorch_intro.py`\n", "- Your function should use Adam to optimize the GP hyperparameters\n", "- You can use the code from the lecture as a guide\n", "\n", "You should see a much tighter fit to the data, with large increases in uncertainty where we do not have data. "], "outputs": []}, {"cell_type": "code", "execution_count": 19, "metadata": {"id": "ttbh3w6Fhwqb", "outputId": "070988a6-9615-4091-c2a3-4d6ce682127e"}, "outputs": [], "source": ["# Define the RBF kernel GP model\n", "rbf_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n", "rbf_model = RBF_GP(train_x, train_y, rbf_likelihood)\n", "\n", "# Optimize the hyperparameters\n", "train_gp_hyperparams(rbf_model, rbf_likelihood, train_x, train_y, lr=0.1)\n", "\n", "# Plot the resulting fit GP\n", "plot_gp_predictions(rbf_model, rbf_likelihood, train_x, train_y, test_x, title='RBF kernel')"]}, {"cell_type": "markdown", "metadata": {"id": "tnIZhJF4hwqb"}, "source": ["### 1.2 Using a polynomial Kernel (5 points)\n", "\n", "Now let's try the Polynomial kernel. This is a kernel of the form\n", "\n", "$$k(x, x') = (x^Tx' + c)^d$$\n", "\n", "Where $d$ is the degree, and $c$ is a kernel hyperparameter.  \n", "\n", "We would use this kernel if we suspected a polynomial of degree $d$ could fit our data well. \n", "\n", "You should do the following\n", "- Complete the function `PolynomialGP.__init__`\n", "- You should use a zero-mean function\n", "- You should use a Polynomial Kernel\n", "- You should compose your polynomial kernel with a scale kernel as you did in the previous question with the RBF kernel\n", "\n"], "outputs": []}, {"cell_type": "code", "execution_count": 20, "metadata": {"id": "5pFv4eckhwqc", "outputId": "fd97edf3-c4c1-4868-9284-c70e5aeed1f1"}, "outputs": [], "source": ["# Let's look and how these kernels perform for two different degrees\n", "\n", "for degree in [4, 5]:\n", "    # Now let's try a Polynomial Kernel\n", "    polynomial_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n", "    polynomial_model = PolynomialGP(train_x, train_y, polynomial_likelihood, degree=degree)\n", "\n", "    # train it\n", "    train_gp_hyperparams(polynomial_model, polynomial_likelihood, train_x, train_y, lr=0.1)\n", "\n", "    # Plot\n", "    plot_gp_predictions(polynomial_model, polynomial_likelihood, train_x, train_y, test_x, title=f'Polynomial Kernel of Degree {degree}')\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "hP3YoLP9hwqc"}, "source": ["From above we can see that a polynomial of degree 4 seems to underfit the data, while a polynomial of degree 5 seeems to overfit the data. For a polynomial of degree 5, the uncertainty seems to actually decrease away from the data!\n", "\n", "While GPs are generally robust to overfitting, we can see that an improper choice of kernel can lead to both poor overfitting and underfitting. "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5qCafdo9hwqc"}, "source": ["## 1.3 Linear Kernel + Cosine Kernel\n", "\n", "We can compose kernels by addition. Thus \n", "$$k(x, x') = k_1(x, x') + k_2(x, x')$$ \n", "\n", "is a valid kernel which we can use in our GP, provided $k_1$ and $k_2$ are kernels.\n", "\n", "We will use the following two kernels:\n", "1. The Linear Kernel $k(x, x') = x^T x' + c$\n", "2. The Cosine Kernel $k(x, x') = \\cos \\left( \\pi \\frac{||x - x'||_2}{p} \\right) $\n", "\n", "$c$ and $p$ are both kernel hyperparameters. Using a linear kernel is a good choice if you suspect that a linear function will fit the data well, and a cosine kernel is a good choice for periodic data. \n", "\n", "You can compose kernels additively in GPytorch using the `+` operator on two kernel instances.\n", "\n", "You should do the following\n", "- Complete `LinearCosineGP.__init__` \n", "- You should use a zero-mean function\n", "- You should additively compose a Linear Kernel and a Cosine Kernel, and compose this combined kernel with the Scale kernel as you have done in the previous questions. The combined kernel should be $\\sigma^2(k_{Linear}(x, x') + k_{Cosine}(x, x'))$"], "outputs": []}, {"cell_type": "code", "execution_count": 21, "metadata": {"id": "RB_UCrYvhwqd", "outputId": "7a6d4e39-e649-4f5e-db39-15b19d6c4e5f"}, "outputs": [], "source": ["# Now let's try a Linear + Cosine Kernel\n", "lincos_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n", "lincos_model = LinearCosineGP(train_x, train_y, lincos_likelihood)\n", "\n", "# train it\n", "train_gp_hyperparams(lincos_model, lincos_likelihood, train_x, train_y, lr=0.1)\n", "\n", "# Plot\n", "plot_gp_predictions(lincos_model, lincos_likelihood, train_x, train_y, test_x, title='Linear + Cosine kernel')\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "kVhiERsyhwqd"}, "source": ["Using the Linear + Cosine Kernel produces a good fit to the data, and a reasonable extrapolation. In fact, in this instance the GP captures the underlying function meaning this extrapolation is very accurate. \n", "\n", "However, note that our original function is an additive composition of a linear term and a periodic term, thus the linear cosine kernel can perform very well. Choosing a very specific kernel is only possible when we have a lot of prior knowledge about the underlying function. \n", "\n", "The RBF kernel is often a good choice when we have less prior knowledge, as it can fit the data well while giving high uncertainty in low-data regions of the input space. "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "OImsVabahwqd"}, "source": ["# 2. Learning Planar Pushing Dynamics with Uncertainty\n", "\n", "For this question you will learn a model which will be used with MPPI so that a robot can push an object to a goal. Unlike in homework 3, however, you will be using learning methods which incorporate model uncertainty. You will also train a model on a much smaller dataset. \n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "qwst10V5hwqe"}, "source": ["### State Space and Action Space\n", "\n", "We will use a similar system from homework 3, with some slight modification. Instead of pushing a cube, the robot will now push a cylinder. For the planar pushing task, we have the following action and state spaces:\n", "\n", "The state space is given by the cylinder position on the table\n", "$$\n", "\\mathbf x = \\begin{bmatrix} x & y \\end{bmatrix}^\\top\n", "$$\n", "Note that the state contains only position elements, and not velocity terms. This is because we assume that the pushing task is *quasi-static*. This means that the pushing actions are slow enough that the velocity and inertia of the cylinder are negligible. In other words, if the robot stops pushing, the cylinder also stops.\n", "\n", "The action space is shown in the diagram below\n", "![Action Space](https://drive.google.com/uc?export=view&id=1uaLhDpowj0OhjNDeYBhkUqWwWv049I93)\n", "\n", "\n", "Each action $\\mathbf u = \\begin{bmatrix} p & \\phi & \\ell\\end{bmatrix}^\\top\\in \\mathbb R^3$ is composed by:\n", "* $p \\in [-0.25 \\pi, 0.25 \\pi]$: pushing location along the edge of the cylinder in radians.  \n", "* $\\phi \\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]$ pushing angle.\n", "* $\\ell\\in [0,1]$ pushing length as a fraction of the maximum pushing length. The maximum pushing length is is 0.1 m\n", "\n", "There is a small amount of noise on the action when applied in the simulator. This means that the dynamics have a small element of randomness in them. \n", "\n", "### Gym Environments\n", "\n", "Our planar pushing task has been wrapped into a `gym` environment. You can find more information about gym environments [here](https://gymnasium.farama.org/api/env/).\n", "\n", "As a gym enviroment, our pushing enviroment has the following useful methods:\n", "\n", "* `step`: Given an action vector, it performs the action in the simulator and returns \n", "    1. `state`: The resulting state, i.e. $x_{t+1}$\n", "    2. `reward`: Not used here (useful for Reinforcement Learning tasks).\n", "    3. `done`: Boolean. In our case the simulation is done (`done=True`) if the robot has reached the goal location or if the block has left the bounds of the workspace.\n", "    4. `info`: Dictionary containing additional data. Not used here.\n", "Example:\n", "```python\n", "state, reward, done, info = env.step(action_i)\n", "```\n", "* `reset`: Resets the simulation to the initial state. It returns the inital state after reset.\n", "Example:\n", "```python\n", "    state = env.reset()\n", "```\n", "\n", "Moreover, our pushing environment has the following attributes:\n", "* `env.action_space`: Represents the action space, following the described parametrization above.\n", "* `env.observation_space`: Represents the state space\n", "These are `gym.spaces` and therefore you can sample them using `.sample()`.\n", "Example:\n", "```python\n", "action_i = env.action_space.sample()\n", "```\n", "This produces actions uniformly sampled from the action space.\n", "\n", "You can find more info about gym spaces [here](https://gymnasium.farama.org/api/spaces/)."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "EmWtnQmPhwqe"}, "source": ["Running the code below will visualize the robot pushing the cylinder"], "outputs": []}, {"cell_type": "code", "execution_count": 22, "metadata": {"colab": {"referenced_widgets": ["7a720be7993443c392be6cddc824cd6a"]}, "id": "_r7sUjxbhwqe", "outputId": "43e39df7-7f22-4bfc-bcbf-b48fc9163db0"}, "outputs": [], "source": ["from panda_pushing_env import PandaDiskPushingEnv\n", "from visualizers import GIFVisualizer, NotebookVisualizer\n", "from tqdm.notebook import tqdm\n", "\n", "%matplotlib inline\n", "\n", "# Create the visualizer\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "# Initialize the simulation environment\n", "env = PandaDiskPushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800)\n", "env.reset()\n", "\n", "\n", "# Perform a sequence of 3 random actions:\n", "for i in tqdm(range(3)):\n", "    action_i = env.action_space.sample()\n", "    state, reward, done, info = env.step(action_i)\n", "    if done:\n", "        break\n", "\n", "plt.close(fig)"]}, {"cell_type": "markdown", "metadata": {"id": "WbGy5BJmhwqf"}, "source": ["## 2.1 Train a GP model of the pushing dynamics (15 points)\n", "\n", "For this part we will train a GP model to approximate the pushing dynamics. We have given you training and validation datasets `pushing_training_data.npy` and `pushing_validation_data.npy`. We will use the residual dynamics formulation\n", "\n", "$$ \\mathbf x_{t+1} = \\mathbf x_t + f(\\mathbf x_t, \\mathbf u_t)$$\n", "\n", "We can make our GP fit the residual dynamics by using the mean function $m(\\mathbf x_t, \\mathbf u_t) = \\mathbf x_t$\n", "\n", "Since our state is 2-dimensional, we must use a multi-output GP. As per the lecture slides, we will be treating each output dimension independently. You can find how to do this in GPytorch [here](https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Batch_Independent_Multioutput_GP.html).\n", "- Complete `ResidualMean.forward` which should compute the mean as specified above. Note the shape information in the docstring!\n", "- Note that the input to this class is `batch_shape`. This should be the number of GPs we have (1 per output dimension). We have kept this naming convention to be consistent with GPytorch, but be aware this is not the same way we have used 'batch' in the past, i.e. number of independent data points. \n", "- Complete `MultiTaskGPModel.__init__` by assigning `mean_module` and `covar_module`. You should use the residual mean function you implemented, and an RBF kernel with a scale kernel. For the RBF kernel, you should use `ard_num_dims=5` as an argument. This means rather than using a single lengthscale in the kernel, each dimension of the input will have its own lengthscale. \n", "- Complete the `PushingDynamicsGP.forward` function\n", "- Notice that we have given you a `PushingDynamics.predict` function. Whenever you are using the model to predict at test time, you should use this function instead of the `PushingDynamics.forward` function\n", "- Complete the `train_dynamics_gp_hyperparameters` function to train the GP hyperparameters. \n", "\n", "You should be able to train using the dataset we provided for you achieving a final loss of below -2.9. "], "outputs": []}, {"cell_type": "code", "execution_count": 23, "metadata": {"id": "XI9QkNwdhwqf", "outputId": "85509b7f-6979-4e05-ba6b-7a3cf88f34a1"}, "outputs": [], "source": ["from learning_uncertain_dynamics import PushingDynamicsGP, MultitaskGPModel, train_dynamics_gp_hyperparams\n", "\n", "\n", "# Load the training data\n", "train_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'pushing_training_data.npy'), allow_pickle=True)\n", "\n", "# Convert to torch\n", "train_states = torch.from_numpy(np.stack([d['states'] for d in train_data], axis=0))\n", "train_actions = torch.from_numpy(np.stack([d['actions'] for d in train_data], axis=0))\n", "\n", "# Convert to states and next_states format\n", "train_next_states = train_states[:, 1:].reshape(-1, 2) \n", "train_states = train_states[:, :-1].reshape(-1, 2)\n", "train_actions = train_actions.reshape(-1, 3)\n", "\n", "# We use a multitask Gaussian likelihood\n", "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2,\n", "                                                             noise_constraint=gpytorch.constraints.GreaterThan(1e-9)\n", "                                                             )\n", "pushing_gp_model = PushingDynamicsGP(train_states, train_actions, train_next_states, likelihood)\n", "train_dynamics_gp_hyperparams(pushing_gp_model, likelihood, train_states, train_actions, train_next_states, 0.1)\n"]}, {"cell_type": "markdown", "metadata": {"id": "1xbMkqt7hwqf"}, "source": ["## 2.2 Train a Neural Network Ensemble model of the pushing dynamics (15 points)\n", "\n", "Now we will train a Neural Network ensemble to approximate the pushing dynamics. We will train N different models of the form \n", "\n", "$$ \\mathbf x_{t+1} = \\mathbf x_t + f(\\mathbf x_t, \\mathbf u_t)$$\n", "\n", "- Complete `SingleStepDynamicsDataset` in `learning_dynamics_gp.py`. You should be able to reuse your code from homework 3\n", "- Complete `ResidualDynamicsModel` in `learning_dynamics_gp.py`. You should be able to use your code from homework 3 with small modifications\n", "- Complete `DynamicsNNEnsemble.forward` to make a prediction for every network in the ensemble. \n", "- Complete `DynamicsNNEnsemble.predict`. This function should return the mean and covariance of the ensemble predictions. You should use the function `batch_cov` from `utils.py` to compute the covariance.\n", "- Train the NN Ensemble in the notebook below. You will then upload the trained model with your code to autograder. \n"], "outputs": []}, {"cell_type": "code", "execution_count": 24, "metadata": {"colab": {"referenced_widgets": ["be1c97828c184514938e2ae632658ab0"]}, "id": "ux4BECsYhwqg", "outputId": "1861e405-33b2-4dce-c570-d3ed6e94aa9d"}, "outputs": [], "source": ["from torch.utils.data import DataLoader\n", "from learning_uncertain_dynamics import SingleStepDynamicsDataset, DynamicsNNEnsemble\n", "from torch.nn import functional as F\n", "\n", "train_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'pushing_training_data.npy'), allow_pickle=True)\n", "validation_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'pushing_validation_data.npy'), allow_pickle=True)\n", "\n", "# Datasets and Dataloaders\n", "train_dataset = SingleStepDynamicsDataset(train_data)\n", "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n", "validation_dataset = SingleStepDynamicsDataset(validation_data)\n", "val_loader = DataLoader(validation_dataset, batch_size=len(validation_dataset))\n", "\n", "\n", "# Train the dynamics model\n", "pushing_nn_ensemble_model = DynamicsNNEnsemble(2, 3, 10)\n", "\n", "\n", "# --- Your code here\n", "\n\n\n", "# ---\n", "\n", "# save model:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'dynamics_nn_ensemble_model.pt')\n", "torch.save(pushing_nn_ensemble_model.state_dict(), save_path)\n"]}, {"cell_type": "markdown", "metadata": {"id": "EDkTrDvKhwqg"}, "source": ["## GP vs NN Ensemble\n", "We can compare the performance of the NN ensemble and the GP by looking at the log likelihood of the validation data. You should see that the GP has log likelihood of above 6, whereas the NN ensemble should have a log likelihood of above 4. \n", "\n", "As a general rule, we would expect the GP to outperform the NN ensemble with a very small dataset. As we increase the size of the dataset, we would expect the NN ensemble to outperform the GP. "], "outputs": []}, {"cell_type": "code", "execution_count": 33, "metadata": {"id": "QpntLTRihwqh", "outputId": "f00feaf8-94db-4f0c-a0ec-52c88374df10"}, "outputs": [], "source": ["from torch.distributions import MultivariateNormal\n", "\n", "# Load validation data\n", "validation_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'pushing_validation_data.npy'), allow_pickle=True)\n", "\n", "# Convert to torch\n", "val_states = torch.from_numpy(np.stack([d['states'] for d in validation_data], axis=0))\n", "val_actions = torch.from_numpy(np.stack([d['actions'] for d in validation_data], axis=0))\n", "\n", "# Convert to states and next_states format\n", "val_next_states = val_states[:, 1:].reshape(-1, 2) \n", "val_states = val_states[:, :-1].reshape(-1, 2)\n", "val_actions = val_actions.reshape(-1, 3)\n", "\n", "# must put to eval mode\n", "pushing_gp_model.eval()\n", "likelihood.eval()\n", "\n", "# Predict with the GP\n", "mu_gp, sigma_gp = pushing_gp_model.predict(val_states, val_actions)\n", "\n", "# predict with the NN \n", "mu_nn, sigma_nn = pushing_nn_ensemble_model.predict(val_states, val_actions)\n", "\n", "\n", "# Compute log likelihoods\n", "gp_loglikelihood = MultivariateNormal(mu_gp, covariance_matrix=sigma_gp).log_prob(val_next_states).mean()\n", "nn_loglikelihood = MultivariateNormal(mu_nn, covariance_matrix=sigma_nn).log_prob(val_next_states).mean()\n", "\n", "print(f'Log-likelihood of the validation data for the GP: {gp_loglikelihood}')\n", "print(f'Log-likelihood of the validation data for the NN Ensemble: {nn_loglikelihood}')"]}, {"cell_type": "markdown", "metadata": {"id": "i0KXECMrhwqh"}, "source": ["Below we show a visualization of the predicted uncertainties of the GP and the ensemble for a single prediction. Since the dynamics are random, we show different sampled ground truth transitions in blue. "], "outputs": []}, {"cell_type": "code", "execution_count": 53, "metadata": {"id": "nLh_PwpIhwqh", "outputId": "cd420456-cd29-4936-9b70-e1d42c486576"}, "outputs": [], "source": ["# one step plotting comparison GP vs NN ensemble\n", "\n", "from utils import plot_uncertainty_propagation\n", "\n", "# Load validation data\n", "d = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'visualization_data.npz'), allow_pickle=True)\n", "states = torch.from_numpy(d['states']).to(torch.float32)\n", "actions = torch.from_numpy(d['actions']).to(torch.float32)\n", "\n", "# initial mean and sigma\n", "pred_mu_GP = [states[0, 0].clone().reshape(1, 2)]\n", "pred_sigma_GP = [torch.zeros(1, 2, 2)]\n", "pred_mu_nn = [states[0, 0].clone().reshape(1, 2)]\n", "pred_sigma_nn = [torch.zeros(1, 2, 2)]\n", "\n", "# Predict next mean and sigma with GP and NN\n", "with torch.no_grad():\n", "    mu_gp, sigma_gp = pushing_gp_model.predict(pred_mu_GP[-1].reshape(-1, 2), actions[0].reshape(-1, 3))\n", "    mu_nn, sigma_nn = pushing_nn_ensemble_model.predict(pred_mu_nn[-1].reshape(-1, 2), actions[0].reshape(-1, 3))\n", "        \n", "    pred_mu_GP.append(mu_gp)\n", "    pred_sigma_GP.append(sigma_gp)  \n", "    pred_mu_nn.append(mu_nn)\n", "    pred_sigma_nn.append(sigma_nn)\n", "    \n", "pred_mu_GP = torch.cat(pred_mu_GP, dim=0)\n", "pred_sigma_GP = torch.cat(pred_sigma_GP, dim=0)\n", "pred_mu_nn = torch.cat(pred_mu_nn, dim=0)\n", "pred_sigma_nn = torch.cat(pred_sigma_nn, dim=0)\n", "\n", "# Plot corresponding ellipses\n", "plot_uncertainty_propagation(states[:, :2], pred_mu_GP, pred_sigma_GP, title='GP single-step prediction')\n", "plot_uncertainty_propagation(states[:, :2], pred_mu_nn, pred_sigma_nn, title='NN single-step prediction')\n"]}, {"cell_type": "markdown", "metadata": {"id": "sh4txCephwqi"}, "source": ["## 3. Uncertainty Propagation with uncertain dynamics (30 points)\n", "\n", "In this part of the question you will implement three different approaches for propagating uncertainty through uncertain nonlinear dynamics. For each of these cases we will approximate the distribution of the state at time $t$ as a multivariate Gaussian $p(x_t) = \\mathcal{N}(\\mu_t, \\Sigma_t) $. \n", "\n", "While the GP dynamics assumes each output is independent, you will see that some of these uncertainty propagation techniques introduce correlations between output dimensions over multiple timesteps. For this reason we will be using full $2\\times 2$ covariance matrices to represent the covariance of $x_t$\n", "\n", "Note that for all of these methods you should use `self.predict` rather than `self.forward`, as `self.predict` will return a mean and covariance matrix.\n", "\n", "\n", "### 3.1 Certainty Equivalence (propagate the mean) (10 points)\n", "\n", "Certainty equivalence is the process of propagating the mean through the non-linear dynamics. At time $t$ we have $x_t \\sim p(x_t) = \\mathcal{N}(\\mu_t, \\Sigma_t) $. Then to predict the future we evaluate the dynamics using the mean via $\\mu_{t+1}, \\Sigma_{t+1} = f(\\mu_t, u_t)$. \n", "\n", "\n", "- Complete `PushingDynamics.propagate_uncertainty_certainty_equivalence` in `learning_uncertain_dynamics.py`\n", "- Your function should return the predicted mean and covariance matrix as detailed above. \n", "- Note that `PushingDynamicsGP` and `PushingDynamicsNNEnsemble` inherit from `PushingDynamics`. Your function should work for both GP and NN Ensemble dynamics. \n", "\n", "You should expect to see that the ellipses do not necessarily grow over time. The GP prediction should be close to the ground truth predictions, but the NN ensemble may not. \n"], "outputs": []}, {"cell_type": "code", "execution_count": 57, "metadata": {"id": "V1nrV6L4hwqi", "outputId": "62cfacc0-e019-4d8b-b5cf-567da40f0e65"}, "outputs": [], "source": ["from utils import plot_uncertainty_propagation, rollout_uncertain\n", "\n", "d = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'visualization_data.npz'), allow_pickle=True)\n", "states = torch.from_numpy(d['states']).to(torch.float32)\n", "actions = torch.from_numpy(d['actions']).to(torch.float32)\n", "\n", "# set propagation method\n", "pushing_gp_model.propagation_method = 'certainty_equivalence'\n", "pushing_nn_ensemble_model.propagation_method = 'certainty_equivalence'\n", "\n", "# Do predictions\n", "pred_mu_gp, pred_sigma_gp = rollout_uncertain(pushing_gp_model, states[0, 0], actions)\n", "pred_mu_nn, pred_sigma_nn = rollout_uncertain(pushing_nn_ensemble_model, states[0, 0], actions)\n", "\n", "\n", "# Plot corresponding ellipses\n", "plot_uncertainty_propagation(states, pred_mu_gp, pred_sigma_gp, 'GP certainty equivalence')\n", "plot_uncertainty_propagation(states, pred_mu_nn, pred_sigma_nn, 'NN Ensemble certainty equivalence')"]}, {"cell_type": "markdown", "metadata": {"id": "Z9aC5ekohwqi"}, "source": ["## 3.2 Moment matching via sampling (10 points)\n", "\n", "Here at time $t$ we again have $x_t \\sim p(x_t) = \\mathcal{N}(\\mu_t, \\Sigma_t) $. \n", "\n", "To perform this process we must first sample $K$ states  $ \\{x^k_t\\}^K \\sim \\mathcal{N}(\\mu_t, \\Sigma_t)$. \n", "\n", "Next, we evaluate the dynamics at each of these $K$ states to get $K$ Gaussian predictions $\\mu^k_{t+1}, \\Sigma^k_{t+1} = f(x^k_t, u_t)$. \n", "\n", "Then we sample a single state from each of these $K$ Gaussians: $x^k_{t+1} \\sim \\mathcal{N}(\\mu^k_{t+1}, \\Sigma^k_{t+1})$\n", "\n", "Finally, we fit a Gaussian by estimating the mean and covariance of the set of samples $\\{x^k_{t+1}\\}^K$\n", "\n", "- Complete `PushingDynamics.propagate_uncertainty_moment_matching` to perform moment matching as described above\n", "- Your function should work when $\\Sigma_t$ is zero. This indicates that the exact value of $x_t$ is already known, such as for the first timestep where we know the current state. You will have to check if $\\Sigma_t$ is zero and treat is as a special case.\n", "- As with certainty equivalence, this function should work with both the NN ensemble and the GP.\n", "- You can use the class `MultivariateNormal` from `torch.distributions`. You can instantiate a distribution with `MultivariateNormal(mean, covariance_matrix=sigma)`. Note that you should use the keyword `covariance_matrix` - this is because there are different ways of parameterizing the covariance matrix that are sometimes more numerically convenient. \n", "- You can sample from `MultivariateNormal` with `.sample(sample_shape)`, where sample_shape is a tuple. \n", "\n", "You should see that now the uncertainty grows with time. You should also see that the GP has better prediction performance, almost all of the ground truth trajectories being within the uncertainty ellipses. \n"], "outputs": []}, {"cell_type": "code", "execution_count": 58, "metadata": {"id": "wBi2JX0vhwqi", "outputId": "025f747b-5a1d-446b-c2b6-d5648034dd45"}, "outputs": [], "source": ["from utils import plot_uncertainty_propagation, rollout_uncertain\n", "\n", "d = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'visualization_data.npz'), allow_pickle=True)\n", "states = torch.from_numpy(d['states']).to(torch.float32)\n", "actions = torch.from_numpy(d['actions']).to(torch.float32)\n", "\n", "# set propagation method\n", "pushing_gp_model.propagation_method = 'moment_matching'\n", "pushing_nn_ensemble_model.propagation_method = 'moment_matching'\n", "\n", "# Do predictions\n", "pred_mu_gp, pred_sigma_gp = rollout_uncertain(pushing_gp_model, states[0, 0], actions)\n", "pred_mu_nn, pred_sigma_nn = rollout_uncertain(pushing_nn_ensemble_model, states[0, 0], actions)\n", "\n", "\n", "# Plot corresponding ellipses\n", "plot_uncertainty_propagation(states, pred_mu_gp, pred_sigma_gp, 'GP moment matching')\n", "plot_uncertainty_propagation(states, pred_mu_nn, pred_sigma_nn, 'NN Ensemble moment matching')"]}, {"cell_type": "markdown", "metadata": {"id": "DzGrQ1DEhwqj"}, "source": ["## 3.3 Linearization (10 points)\n", "\n", "The linearization technique exploits the fact that a linear function of a Gaussian random vector is itself a Gaussian random vector. Consider $\\mathbf X \\sim \\mathcal{N}(\\mu, \\Sigma)$, and $\\mathbf Y = A \\mathbf X$\n", "\n", "Then $\\mathbf Y$ is also a Gaussian random vector, with mean given by\n", "\n", "$$ \\mathbb{E}[\\mathbf Y] = \\mathbb{E}[A \\mathbf X] = A \\mathbb{E}[ \\mathbf X ] = A \\mu $$\n", "\n", "and covariance given by \n", "\n", "\\begin{align}\n", "\\text{Cov}(\\mathbf Y) &= \\mathbb{E}[(Y - \\mathbb{E}[\\mathbf Y])(Y - \\mathbb{E}[\\mathbf Y])^T] \\\\ \n", "&= \\mathbb{E}[(AX - A\\mathbb{E}[\\mathbf X])(AX - A \\mathbb{E}[\\mathbf X])^T] \\\\ \n", "&= A\\mathbb{E}[(X - \\mathbb{E}[\\mathbf X])(X - \\mathbb{E}[\\mathbf X])^T]A^T \\\\ \n", "&= A\\Sigma A^T \\\\ \n", "\\end{align}\n", "\n", "The above equations motivate the use of a linearization of the non-linear mean function in order to propagate the uncertainty. The equations for propagating the uncertainty are given by \n", "\n", "$$ \\mu_{t+1}, \\hat{\\Sigma}_{t+1} = f_{GP}(\\mu_t, u_t) $$\n", "$$ \\Sigma_{t+1} = \\hat{\\Sigma}_{t+1} + A \\Sigma_t A^T$$\n", "\n", "Where $A$ is the linearization of the GP mean function\n", "$$ A = \\frac{\\partial \\mu_{t+1}}{\\partial \\mu_t} (\\mu_t, u_t)$$\n", "\n", "- Complete the function `PushingDynamicsGP.propagate_uncertainty_linearization` using the formula above. \n", "- In principle this linearization can be applied to the NN ensemble - it just requires computing the linearization of the NN ensemble mean prediction. However, we will only consider method for the GP. \n", "- We have given a method for computing $A$ above which is already in your starter code\n", "\n", "You should see that the uncertainty ellipses grow with time. However, comparing the below plot to the moment-matching plot for the GP, you should see that the uncertainty ellipses for the linearization technique are generally smaller. \n"], "outputs": []}, {"cell_type": "code", "execution_count": 55, "metadata": {"id": "HuJkhO27hwqj", "outputId": "af8ef705-8065-428c-9c47-93ab2777e077"}, "outputs": [], "source": ["from utils import plot_uncertainty_propagation, rollout_uncertain\n", "\n", "d = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'visualization_data.npz'), allow_pickle=True)\n", "states = torch.from_numpy(d['states']).to(torch.float32)\n", "actions = torch.from_numpy(d['actions']).to(torch.float32)\n", "\n", "# set propagation method\n", "pushing_gp_model.propagation_method = 'linearization'\n", "\n", "# Do predictions\n", "pred_mu_gp, pred_sigma_gp = rollout_uncertain(pushing_gp_model, states[0, 0], actions)\n", "\n", "\n", "# Plot corresponding ellipses\n", "plot_uncertainty_propagation(states, pred_mu_gp, pred_sigma_gp, 'GP linearization')\n"]}, {"cell_type": "markdown", "metadata": {"id": "3G-tYaLuhwqj"}, "source": ["# 4. Using Uncertain Dynamics with MPPI (20 points)\n", "\n", "## 4.1 Obstacle Free Case (5 points)\n", "\n", "Now you will use the GP model and the uncertainty propagation techniques with MPPI to push the object to the goal. \n", "\n", "Note that all the uncertainty propagation techniques previously took the form of\n", "\n", "$$ \\mu_{t+1}, \\Sigma_{t+1} = f(\\mu_t, \\Sigma_t, u_t) $$\n", "\n", "To easily incorporate this within our MPPI implementation we will be performing MPPI with an augmented state, where the state in MPPI will be $(\\mu, \\Sigma)$ and the actions remain $u$. \n", "\n", "We will use the following cost function to perform MPPI:\n", "\n", "$$\\text{Cost}(\\mu_t, \\Sigma_t, u_t) = (\\mu_t - x_{goal})^T(\\mu_t - x_{goal}) + \\text{trace} (\\Sigma_t)$$\n", "\n", "Why do we use this cost function? Intuitively trajectories minimizing this cost function will reach the goal while also having low overall uncertainty.\n", "\n", "It is also equivalent to minimizing the **Expected Cost**, since we have that for any random vector $x$ with mean $\\mu$ and covariance $\\Sigma$ we have\n", "\n", "$$\\mathbb{E}\\left[ \\mathbf x^T Q \\mathbf x \\right ] = \\mu^T Q \\mu + \\text{trace}( Q \\Sigma) $$\n", "\n", "This means we can interpret minimizing the cost function above as finding trajectories that have low cost **on average**. An explanation for the above formula for expected cost can be found [here](https://stats.stackexchange.com/questions/48066/expected-value-of-quadratic-form)\n", "\n", "\n", "\n", "\n", "- Complete the functions `PushingController._compute_dynamics` and `PushingController.control`. \n", "- `_compute_dynamics` should use `self.model.propagate_uncertainty` to propagate the uncertainty. The state in MPPI will be ($\\mu, \\Sigma$), where $\\Sigma$ has been reshaped into a vector of length 4. \n", "- Complete `free_pushing_cost_function`\n", "\n"], "outputs": []}, {"cell_type": "code", "execution_count": 24, "metadata": {"colab": {"referenced_widgets": ["2c27420941b54deb9c565b2a6fe8951b"]}, "id": "a5D8E2Afhwqk", "outputId": "32fd50d8-9baa-4257-dc5b-8785b54eb54d"}, "outputs": [], "source": ["# now let's try and do MPPI to get to the goal using our dynamics\n", "# Control on an obstacle free environment\n", "\n", "%matplotlib inline\n", "\n", "from learning_uncertain_dynamics import PushingController, free_pushing_cost_function\n", "\n", "from panda_pushing_env import TARGET_POSE_FREE, DISK_RADIUS\n", "\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "pushing_gp_model.propagation_method = 'certainty_equivalence'\n", "cost = free_pushing_cost_function\n", "env = PandaDiskPushingEnv(visualizer=visualizer, render_non_push_motions=False,  \n", "                      camera_heigh=800, camera_width=800, render_every_n_steps=5, include_obstacle=False)\n", "\n", "controller = PushingController(env, pushing_gp_model, \n", "                               cost, num_samples=100, horizon=10)\n", "env.reset()\n", "\n", "state_0 = env.reset()\n", "state = state_0\n", "\n", "num_steps_max = 20\n", "\n", "env.reset()\n", "for i in tqdm(range(num_steps_max)):\n", "    action = controller.control(state)\n", "    state, reward, done, _ = env.step(action)\n", "    if done:\n", "        break\n", "\n", "\n", "# Evaluate if goal is reached\n", "end_state = env.get_state()\n", "target_state = TARGET_POSE_FREE\n", "goal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) # evaluate only position, not orientation\n", "goal_reached = goal_distance < 1.2*DISK_RADIUS\n", "\n", "print(f'GOAL REACHED: {goal_reached}')\n", "        \n", "        \n", "plt.close(fig)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "Z-X7GWFChwqk"}, "source": ["## 4.2 - Cost Function for Obstacle Avoidance (15 points)\n", "\n", "Now we will add an obstacle to the environment. The obstacle is a purple cylinder on the table. \n", "\n", "### 4.2 a) Collision checking the mean (7 points)\n", "\n", "You should implement a new cost function to reach the goal while avoiding the obstacle. The cost should be\n", "\n", "\n", "$$\\text{Cost}(\\mu_t, \\Sigma_t, u_t) = (\\mu_t - x_{goal})^T(\\mu_t - x_{goal}) + \\text{trace} (\\Sigma_t) + 100 \\texttt{in_collision}(\\mu_t)$$\n", "\n", "With `in_collision` being 1 if the disk is in collision with the obstacle and 0 otherwise. This should add an additional state cost of 100 whenever a state is in collision. Intuitively, minimizing this cost will find trajectories with means that reach the goal and avoid the obstacle, while minimizing the total uncertainty. \n", "\n", "\n", "**TODO:**\n", "\n", "- Implement `obstacle_avoidance_pushing_cost_function` in `learning_state_dynamics.py`.\n", "- As part of this question you will need to implement a method of detecting collision between two 2D circles.\n", "\n", "If implemented successfully, you should see the robot pushing the cylinder around the obstacle to the goal. The robot should complete the task within the 20 steps, but may take 15-20 steps to do so. Even with a good dynamics model the robot may occasionally overshoot the goal, but should succeed more often than not.\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "execution_count": 25, "metadata": {"colab": {"referenced_widgets": ["3c4ca80972104ae89476c2572d5c3682", "adfc0dd9a4dd4315b96dd6e17d76e72d", "5a985a4e333f4377aa406cf3328730be"]}, "id": "3Db6mJ0Jhwqk", "outputId": "c8d40b84-d5d8-4fae-bf4b-e1b75fa62303"}, "outputs": [], "source": ["# now let's try and do MPPI to get to the goal using our dynamics\n", "# Control on an obstacle free environment\n", "\n", "%matplotlib inline\n", "\n", "from learning_uncertain_dynamics import PushingController, obstacle_avoidance_pushing_cost_function\n", "\n", "from panda_pushing_env import TARGET_POSE_OBSTACLES, DISK_RADIUS\n", "\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "pushing_gp_model.propagation_method = 'moment_matching'\n", "cost = obstacle_avoidance_pushing_cost_function\n", "env = PandaDiskPushingEnv(visualizer=visualizer, render_non_push_motions=False,  \n", "                      camera_heigh=800, camera_width=800, render_every_n_steps=5, include_obstacle=True)\n", "\n", "controller = PushingController(env, pushing_gp_model, \n", "                               cost, num_samples=100, horizon=10)\n", "env.reset()\n", "\n", "state_0 = env.reset()\n", "state = state_0\n", "\n", "num_steps_max = 20\n", "\n", "env.reset()\n", "for i in tqdm(range(num_steps_max)):\n", "    action = controller.control(state)\n", "    state, reward, done, _ = env.step(action)\n", "    if done:\n", "        break\n", "\n", "\n", "# Evaluate if goal is reached\n", "end_state = env.get_state()\n", "target_state = TARGET_POSE_OBSTACLES\n", "goal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) # evaluate only position, not orientation\n", "goal_reached = goal_distance < 1.2*DISK_RADIUS\n", "\n", "print(f'GOAL REACHED: {goal_reached}')\n", "        \n", "        \n", "plt.close(fig)"]}, {"cell_type": "markdown", "metadata": {"id": "Vi_PmunAhwqk"}, "source": ["### 4.2 a) Expected cost with samples (8 points)\n", "\n", "In section 4.1 we discussed that the cost we were using was the **Expected Cost**, where the expectation was taken with respect to our state uncertainty. \n", "\n", "In the previous section we only performed collision checking of the mean. Note that \n", "\n", "$$ \\mathbb{E}\\left[ (x - x_{goal})^T(x - x_{goal}) + 100 \\texttt{in_collision}(x)\\right] = (\\mu - x_{goal})^T(\\mu - x_{goal})  + \\text{trace} (\\Sigma) + 100 \\mathbb{E}\\left[\\texttt{in_collision}(x)\\right]$$\n", "\n", "Since the expectation is a linear operator. In the previous question we used $\\texttt{in_collision}(\\mu)$ instead of $\\mathbb{E}\\left[\\texttt{in_collision}(x)\\right]$, but these two are not equal. For instance we might expect that if we are near the obstacle and have high uncertainty, then we have a higher chance of collision. We can instead approximate the collision cost with samples:\n", "\n", "$$ \\{x_k\\}^K \\sim \\mathcal{N}(\\mu, \\Sigma) $$\n", "$$ \\mathbb{E}\\left[\\texttt{in_collision}(x)\\right] \\approx \\frac{1}{K}\\sum^K_{k=1} \\texttt{in_collision}(x_k)$$\n", "\n", "\n", "\n", "**TODO:**\n", "\n", "- Implement `obstacle_avoidance_pushing_cost_function_samples` in `learning_state_dynamics_gp.py`.\n", "- You should use samples to estimate the expectation of the collision detection\n", "- You should use K=10\n", "\n", "\n", "Once you have implemented this, you can run the code below to use the cost function.\n", "\n", "If implemented successfully, you should see the robot pushing the cylinder around the obstacle to the goal. The robot should complete the task within the 20 steps, but may take 15-20 steps to do so. Even with a good dynamics model the robot may occasionally overshoot the goal, but should succeed more often than not.\n", "\n"], "outputs": []}, {"cell_type": "code", "execution_count": 26, "metadata": {"colab": {"referenced_widgets": ["fb0f5a4526b44475a0e751d257c0499c"]}, "id": "xIrUU-srhwql", "outputId": "b6452c83-6cdc-4776-d55a-e46445e0dbd3"}, "outputs": [], "source": ["# now let's try and do MPPI to get to the goal using our dynamics\n", "# Control on an obstacle free environment\n", "\n", "%matplotlib inline\n", "\n", "from learning_uncertain_dynamics import obstacle_avoidance_pushing_cost_function_samples\n", "from panda_pushing_env import TARGET_POSE_OBSTACLES, DISK_RADIUS\n", "\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "pushing_gp_model.propagation_method = 'moment_matching'\n", "cost = obstacle_avoidance_pushing_cost_function_samples\n", "env = PandaDiskPushingEnv(visualizer=visualizer, render_non_push_motions=False,  \n", "                      camera_heigh=800, camera_width=800, render_every_n_steps=5, include_obstacle=True)\n", "\n", "controller = PushingController(env, pushing_gp_model, \n", "                               cost, num_samples=100, horizon=10)\n", "env.reset()\n", "\n", "state_0 = env.reset()\n", "state = state_0\n", "\n", "# num_steps_max = 100\n", "num_steps_max = 20\n", "\n", "env.reset()\n", "for i in tqdm(range(num_steps_max)):\n", "    action = controller.control(state)\n", "    state, reward, done, _ = env.step(action)\n", "    if done:\n", "        break\n", "\n", "\n", "# Evaluate if goal is reached\n", "end_state = env.get_state()\n", "target_state = TARGET_POSE_OBSTACLES\n", "goal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) # evaluate only position, not orientation\n", "goal_reached = goal_distance < 1.2*DISK_RADIUS\n", "\n", "print(f'GOAL REACHED: {goal_reached}')\n", "        \n", "        \n", "plt.close(fig)"]}], "metadata": {"colab": {"provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "venv", "language": "python", "name": "venv"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 1}