{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ugo7EEyP5G-"
   },
   "source": [
    "# ROB 498: Robot Learning for Planning and Control\n",
    "# Assignment 2: Introduction to Optimal Control "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PhImJ85xix9"
   },
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1674939325819,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "GKlU12PTzIZn"
   },
   "outputs": [],
   "source": [
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a ROB498 folder and put all the files under HW2 folder, then 'ROB498/HW1'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ROB498/HW2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcYCH3lWzKto"
   },
   "source": [
    "### Setup Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1674939325820,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "WR9nHjCHxAFE"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247476,
     "status": "ok",
     "timestamp": 1674939573291,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "-d_01Y1yy4E7",
    "outputId": "3f426ad3-84d8-46bb-e8ec-cfca5c0c190d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3701,
     "status": "ok",
     "timestamp": 1674939580399,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "EQdugHRazlsC",
    "outputId": "150a9931-afe4-4a84-e04d-9644d2ef8388"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "\n",
    "files = os.listdir(GOOGLE_DRIVE_PATH)\n",
    "expected_files = [ 'ROB498_hw2.ipynb', 'base_env.py', 'cartpole_env.py', 'panda_env.py', 'linear_mpc.py', 'mppi_control.py'] \n",
    "\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "# Verify that there are all the expected files in the directory\n",
    "all_found = True\n",
    "for expected_file in expected_files:\n",
    "  if expected_file not in files:\n",
    "    print(f'Required file {expected_file} not found!')\n",
    "    all_found = False\n",
    "if all_found:\n",
    "  print('All required files are found :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23401,
     "status": "ok",
     "timestamp": 1674939607213,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "uJYdKikwTTu9",
    "outputId": "9b6c8542-1191-4c1f-c24d-b138c25765d7"
   },
   "outputs": [],
   "source": [
    "# Install missing required packages \n",
    "# Unfortunately Colab does not have pybullet package by default, so we will have to install it every time that the notebook kernel is restarted.\n",
    "# Install pybullet -- For simulation purposes\n",
    "!pip install pybullet\n",
    "# Install numpngw -- For visualization purposes\n",
    "!pip install numpngw\n",
    "!pip install control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3961,
     "status": "ok",
     "timestamp": 1674939614480,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "zr3mO5nTzyBR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from numpngw import write_apng\n",
    "from IPython.display import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77GR7M460IkA"
   },
   "source": [
    "## Assignment Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11n8VqQ6B79C"
   },
   "source": [
    "Due 2/15 at 11:59pm\n",
    "\n",
    "**Rules**:\n",
    "\n",
    "1. All homework must be done individually, but you are encouraged to post questions on Piazza\n",
    "\n",
    "2. No late homework will be accepted (unless you use your late-day tokens)\n",
    "\n",
    "3. Submit your code on [autograder.io](http://autograder.io/)\n",
    "\n",
    "4. Remember that copying-and-pasting code from other sources is not allowed\n",
    "\n",
    "5. The use of additional package imports beyound the packages we provide is not allowed. The autograder will not grade your code if you use additional packages.\n",
    "\n",
    "**Instructions**\n",
    "- Each problem will give you a file with some template code, and you need to fill in the\n",
    "rest.\n",
    "- We use the autograder, so if you’re ever wondering “will I get full points for\n",
    "this?” just upload your code in the autograder to check. There is no limit to how\n",
    "many times you can upload to autograder.\n",
    "- The autograder may test your problem with multiple different inputs to make sure it is correct.\n",
    "- The autograder will only show you if you got it right/wrong, so if you don’t get full points, try to test with some other inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWDwT0WLxqfN"
   },
   "source": [
    "## 1 - Cartpole Control (60 points)\n",
    "\n",
    "In this section you will apply optimal control policies to control one of the classical control problems: the cartpole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHNcp_VqR2ZP"
   },
   "outputs": [],
   "source": [
    "from cartpole_env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng6fUK00SUR_"
   },
   "source": [
    "### Cartpole Setup\n",
    "The cartpole is an example of an underactuated dynamical system. The goal is to mantain the pole location vertically while only being able to control the cart displacement via applying a linear force $F$.\n",
    "\n",
    "Below you will see an example of the cartpole system we will be working with. This cartpole is simulated using PyBullet. Here, we will just apply random forces to the cart.\n",
    "\n",
    "\n",
    "If you want to know more details about the pybullet simulation. You can check the `Cartpole` class in `cartpole_env.py`.\n",
    "\n",
    "Run the cell below and observe that random actions result in behaviour far from stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 7119,
     "status": "ok",
     "timestamp": 1674572764945,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "5yH6QkNXT9zO",
    "outputId": "bf31cb4b-ca7b-4a87-898f-5fe56cdace72"
   },
   "outputs": [],
   "source": [
    "env = CartpoleEnv()\n",
    "env.reset(state = np.array([0.0, 0.5, 0.0, 0.0]))\n",
    "\n",
    "frames=[] #frames to create animated png\n",
    "frames.append(env.render())\n",
    "for i in tqdm(range(100)):\n",
    "    action = env.action_space.sample()\n",
    "    s = env.step(action)\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "\n",
    "write_apng(\"cartpole_example.png\", frames, delay=10)\n",
    "Image(filename=\"cartpole_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nev4NFfQUHqY"
   },
   "source": [
    "## 1.1 Modelling & Linearization (14 points)\n",
    "\n",
    "For this question you will implement an analytical dynamics function for the cartpole in PyTorch and explore different techniques for performing linearization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 a) PyTorch model of cartpole dynamics (6 points)\n",
    "A diagram of the cartpole is shown below.\n",
    "\n",
    "The state vector for the cartpole is $\\mathbf{x} = [x, \\theta, \\dot{x}, \\dot{\\theta}]$. Here $x$ is the horizontal position of the cart, and $\\theta$ is the angle of the pole. $\\dot{x}, \\theta$ are the velocities. The control $\\mathbf{u}$ is the horizontal force $F$ in the $x$ direction. $\\theta=0$ corresponds to the 'pole up' position.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ERKgdsh8H7_0BnPaY1-CO9E_fYg7ASSX\" width=300\"/>\n",
    "</div>\n",
    "\n",
    "The equations of motions are as followed: \n",
    "\n",
    "$$ \\ddot{\\theta} = \\frac{g \\sin \\theta - \\cos \\theta \\left [ \\frac{F + m_p l \\dot{\\theta}^2 \\sin \\theta}{m_c + m_p} \\right ]}{l \\left[ \\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p} \\right ]\n",
    "}$$\n",
    "\n",
    "$$ \\ddot{x} = \\frac{F + m_p l \\left[ \\dot{\\theta}^2 \\sin \\theta - \\ddot{\\theta} \\cos \\theta \\right]}{m_c + m_p}$$\n",
    "\n",
    "Where\n",
    "\n",
    " * $m_p$ is the mass of the pendulum\n",
    " * $m_c$ is the mass of the cart\n",
    " * $l$ is the length of the pendulum\n",
    " * $g$ is the gravity\n",
    "\n",
    "**For this question**:\n",
    "- Complete the function `dynamics_analytic` in `cartpole_env.py` using the equations of motion above\n",
    "- Your function should operate on batches of tensors - this will come in handy later!\n",
    "- The relevant parameter values are already set inside `dynamics_analytic` - you do not need to modify them\n",
    "- You can use `torch.chunk` to split the batched state of shape (B, 4) into 4 (B, 1) shape tensors representing the elements of the state. You can then compute the above accelerations using elementwise operations\n",
    "- You should use compute the next state from accelerations and velocities using the following integration scheme:\n",
    "\n",
    "$$ \\dot{x}_{t+1} = \\dot{x}_t + \\Delta t \\: \\ddot{x}_t$$\n",
    "$$ \\dot{\\theta}_{t+1} = \\dot{\\theta}_t + \\Delta t \\: \\ddot{\\theta}_t$$\n",
    "$$ x_{t+1} = x_t + \\Delta t \\: \\dot{x}_{t+1}$$\n",
    "$$ \\theta_{t+1} = \\theta_t + \\Delta t \\: \\dot{\\theta}_{t+1}$$\n",
    "\n",
    "- Note that the positions are updated with the new velocity rather than the old\n",
    "\n",
    "                                                                                                                                                                                                   \n",
    "**GRADING INFO**:\n",
    "\n",
    "We will be checking the correctness of your dynamics implementation across different batch sizes.                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test to see if your analytic dynamics matches the simulator\n",
    "\n",
    "# first let's generate a random control sequence\n",
    "T = 50\n",
    "control_sequence = np.random.randn(T, 1)\n",
    "start_state = np.array([1, 0.25, 0, 0])\n",
    "\n",
    "# We use the simulator to simulate a trajectory\n",
    "env = CartpoleEnv()\n",
    "env.reset(start_state)\n",
    "states_pybullet = np.zeros((T+1, 4))\n",
    "states_pybullet[0] = start_state\n",
    "for t in range(T):\n",
    "    states_pybullet[t+1] = env.step(control_sequence[t])\n",
    "\n",
    "# Now we will use your analytic dynamics to simulate a trajectory\n",
    "states_analytic = torch.zeros(T+1, 1, 4) # Need an extra 1 which is the batch dimension (T x B x 4)\n",
    "states_analytic[0] = torch.from_numpy(start_state).reshape(1, 4)\n",
    "for t in range(T):\n",
    "    current_state = states_analytic[t]\n",
    "    current_control = torch.from_numpy(control_sequence[t]).reshape(1, 1) # add batch dimension to control    \n",
    "    states_analytic[t+1] = dynamics_analytic(current_state, current_control)\n",
    "    \n",
    "# convert back to numpy for plotting\n",
    "states_analytic = states_analytic.reshape(T+1, 4).numpy()\n",
    "\n",
    "# Plot and compare - They should be indistinguishable \n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "axes[0][0].plot(states_analytic[:, 0], label='analytic')\n",
    "axes[0][0].plot(states_pybullet[:, 0], '--', label='pybullet')\n",
    "\n",
    "axes[0][0].title.set_text('x')\n",
    "axes[0][1].plot(states_analytic[:, 1])\n",
    "axes[0][1].plot(states_pybullet[:, 1], '--')\n",
    "\n",
    "axes[0][1].title.set_text('theta')\n",
    "axes[1][0].plot(states_analytic[:, 2])\n",
    "axes[1][0].plot(states_pybullet[:, 2], '--')\n",
    "\n",
    "axes[1][0].title.set_text('x_dot')\n",
    "axes[1][1].plot(states_analytic[:, 3])\n",
    "axes[1][1].plot(states_pybullet[:, 3], '--')\n",
    "\n",
    "axes[1][1].title.set_text('theta_dot')\n",
    "axes[0][0].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test to see if your batching works as expected\n",
    "\n",
    "# We should see that the analytic dynamics should produce approximately the same next states\n",
    "# as the simulator. We can check this using a single call to your batched analytical dynamics function.\n",
    "\n",
    "B = 4\n",
    "states = 0.1 * torch.randn(B, 4)\n",
    "actions = torch.randn(B, 1)\n",
    "\n",
    "# first lets see what the pybullet dynamics are\n",
    "print('Next states from simulator are: ')\n",
    "for state, action in zip(states, actions):\n",
    "    print(env.dynamics(state.numpy(), action.numpy()))\n",
    "\n",
    "print('')\n",
    "print('Batched next states from analytical dynamics are')\n",
    "print(dynamics_analytic(states, actions))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj6uflV1Ua8t"
   },
   "source": [
    "### 1.1 b) Linearization (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 16849,
     "status": "ok",
     "timestamp": 1674573603197,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "dt61tI8WUdhE",
    "outputId": "13544d25-570b-4a80-f2e4-1bafb9d944f3"
   },
   "source": [
    "In this section you will use two different techniques for linearizing the Cartpole dynamics around the fixed point $\\mathbf{x}_r = [0, 0, 0, 0]^T, \\mathbf{u}_r = [0]$. \n",
    "\n",
    "When linearizing about a reference point $(\\mathbf{x}_r, \\mathbf{u}_r)$ the linearized dynamics are given by:\n",
    "\n",
    "$$ \\mathbf{x}_{t+1} = \\mathbf{x}_r + A(\\mathbf{x}_r, \\mathbf{u}_r) (\\mathbf{x}_t - \\mathbf{x}_r) + B(\\mathbf{x}_r, \\mathbf{u}_r) (\\mathbf{u}_t - \\mathbf{u}_r)$$\n",
    "\n",
    "For the linearization at $\\mathbf{x}_r = [0, 0, 0, 0]^T, \\mathbf{u}_r = [0]$. This simplifies to:\n",
    "\n",
    "$$ \\mathbf{x}_{t+1} = A(0,0)\\mathbf{x}_t + B(0,0)\\mathbf{u}_t$$\n",
    "\n",
    "\n",
    "\n",
    "#### Part i)\n",
    "\n",
    "- Complete the function `CartpoleEnv.linearize_numerical` in `cartpole.py`. \n",
    "- Your should use the Symmetric Different Quotient formula from the lecture slides to compute the numerical derivatives. \n",
    "- You can use the function `CartpoleEnv.dynamics` to evaluate the cartpole dynamics for a given state and control.\n",
    "\n",
    "#### Part ii)\n",
    "- Complete the function `linearize_pytorch` in `cartpole.py`. \n",
    "- You should use your `dynamics_analytic` function from the previous question.\n",
    "- `linearize_pytorch` expects tensors of shape (4,) and (1,). This is in contrast to `dynamics_analytic` which expects tensors of shape (B, 4) and (B, 1). You should use your `dynamics_analytic` function with B=1. You can add and remove dimensions with `torch.unsqueeze`, `torch.squeeze`, or `torch.reshape`\n",
    "\n",
    "\n",
    "Note: You should notice that the $A$ and $B$ matrices produced by these functions are very similar, but not identical. They may be different by an order of approximately $1 \\times 10^{-4}$\n",
    "\n",
    "**GRADING INFO**:\n",
    "\n",
    "We will check your linearization functions for the cartpole at different linearization points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearization around \n",
    "A_numerical, B_numerical = env.linearize_numerical(np.zeros(4), np.zeros(1))\n",
    "\n",
    "print('Numerical Linearizations are ')\n",
    "print(A_numerical)\n",
    "print(B_numerical)\n",
    "print('')\n",
    "A_autograd, B_autograd = linearize_pytorch(torch.zeros(4), torch.zeros(1))\n",
    "\n",
    "print('Autograd linearizations are ')\n",
    "print(A_autograd)\n",
    "print(B_autograd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTkXqH6zUeSI"
   },
   "source": [
    "## 1.2 Linear MPC\n",
    "\n",
    "In this section we will use the linearization you have developed above to synthesise a few different controllers. Our goal is to stabilize the cartpole around the 'pole-up' position.  \n",
    "\n",
    "#### Note on implementation\n",
    "\n",
    "For the remainder of this question, note from the docstrings that these functions are expected to take in and return NumPy arrays rather than torch.tensors. The reason for this is because the tools we will use in this question (`control`, `cvxpy` and `pybullet`) work with NumPy and not PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_mpc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLPRNs5iU9Il"
   },
   "source": [
    "### 1.2 a) Finite Horizon LQR without constraints (16 points)\n",
    "\n",
    "For finite horizon LQR our goal is to solve the following problem from some start state $\\mathbf{x}_0$: \n",
    "\n",
    "$$ \\min_{\\mathbf{u}_0, ... \\mathbf{u}_{H-1}} \\sum^H_{t=1} \\mathbf{x_t}^T Q \\mathbf{x_t} + \\mathbf{u_{t-1}}^T R \\mathbf{u_{t-1}}$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$ \\mathbf{x} _{t+1} = A \\mathbf{x}_t + B \\mathbf{u}_t $$\n",
    "\n",
    "We can solve this problem via Least squares, reformulating to remove the dynamics constraint and all states from the optimization. First we note that:\n",
    "$$ \\mathbf{x}_{1} = A \\mathbf{x}_0 + B \\mathbf{u}_0 $$\n",
    "$$ \\mathbf{x}_{2} = A^2 \\mathbf{x}_0 + A B \\mathbf{u}_0 + B \\mathbf{u}_1$$\n",
    "\n",
    "Where $A^2$ is a matrix power. We can do this for the entire trajectory resulting in: \n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_2 \\\\ \n",
    "\\vdots\\\\\n",
    "\\mathbf{x}_H\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "B & 0 & \\cdots & 0 \\\\\n",
    "AB & B & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A^{H-1}B & A^{H-2}B & \\dots & B\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{u}_0\\\\\n",
    "\\mathbf{u}_1\\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{u}_{H-1}\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "\\begin{bmatrix}\n",
    "A\\\\\n",
    "A^2\\\\\n",
    "\\vdots \\\\\n",
    "A^H\n",
    "\\end{bmatrix}\\mathbf{x}_0\n",
    "$$\n",
    "\n",
    "We rewrite this as: \n",
    "$$ \\mathbf{X} = S \\mathbf{U} + M \\mathbf{x}_0$$\n",
    "\n",
    "#### Part i) Compute $S$ and $M$ matrices (6 points)\n",
    "You should do the following:\n",
    "- Complete `LinearMPC.compute_S_and_M` in `linear_mpc.py`\n",
    "- Hint: You can manually compute $A^{n}$, and $A^{n}B$ and compare it to the entries of $S$ and $M$. It's easier if you do this for a short horizon for debugging purposes\n",
    "\n",
    "**GRADING INFO**:\n",
    "\n",
    "We will be checking your `compute_S_and_M` function for $A$ and $B$ matrices of different sizes and for different horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get linearization\n",
    "A, B = env.linearize_numerical(np.zeros(4), np.zeros(1))\n",
    "\n",
    "# Q and R matrices\n",
    "Q = np.array([\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.1, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.1]\n",
    "])\n",
    "\n",
    "R = np.array([[0.1]])\n",
    "\n",
    "# Note -- To check your implementation you should modify the horizon\n",
    "# and check that the entries match your expectations\n",
    "controller = LinearMPC(A, B, Q, R, horizon=3)\n",
    "S, M = controller.compute_SM()\n",
    "print('S matrix is:')\n",
    "print(S)\n",
    "print('M matrix is:')\n",
    "print(M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYN1PyV5U-ZE"
   },
   "source": [
    "#### Part ii) Solving Finite horizon LQR with Least Squares (10 points)\n",
    "Using the above matrices we can reformulate the finite horizon LQR as the following: \n",
    "\n",
    "$$ \\min_\\mathbf{U} (S\\mathbf{U} + M\\mathbf{x}_0)^T \\bar{Q}(S\\mathbf{U} + M\\mathbf{x}_0) + \\mathbf{U}^T \\bar{R} \\mathbf{U}$$\n",
    "\n",
    "Where \n",
    "$$ \\bar{Q} = \n",
    "\\begin{bmatrix}\n",
    "Q & 0 & \\cdots & 0 \\\\\n",
    "0 & Q & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & Q\n",
    "\\end{bmatrix}\n",
    "\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\n",
    "\\bar{R} = \n",
    "\\begin{bmatrix}\n",
    "R & 0 & \\cdots & 0 \\\\\n",
    "0 & R & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & R\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The problem above turns out to be an unconstrained least squares solution. \n",
    "\n",
    "We can solve this by setting the derivative to zero:\n",
    "\n",
    "$$ \\nabla_\\mathbf{U} \\left[ (S\\mathbf{U} + M\\mathbf{x}_0)^T \\bar{Q}(S\\mathbf{U} + M\\mathbf{x}_0) + \\mathbf{U}^T \\bar{R} \\mathbf{U} \\right] = 0$$\n",
    "\n",
    "Solving this and rearranging results in:\n",
    "\n",
    "$$\\mathbf{U} = - (S^T \\bar{Q} S + \\bar{R})^{-1} S^T \\bar{Q} M \\mathbf{x}_0$$\n",
    "\n",
    "The solution can be written as $G \\mathbf{x}_0$ for $G$ given by:\n",
    "$$G = - (S^T \\bar{Q} S + \\bar{R})^{-1} S^T \\bar{Q} M = \\begin{bmatrix}\n",
    "G_0 \\\\\n",
    "G_1 \\\\\n",
    "\\vdots \\\\\n",
    "G_{H-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first action is then given by $\\mathbf{u}_0 = G_0 \\mathbf{x}_0$\n",
    "\n",
    "Since $G_0$ is not dependent on the current state, it turns out that using a feedback policy of $\\mathbf{u} = G_0 \\mathbf{x}$ is equivalent to performing this finite-horizon LQR optimization at every timestep, and taking the first optimal action. \n",
    "\n",
    "You can find more details about this method of solving the finite horizon LQR problem in Chapter 7 of:\n",
    "\n",
    "Borrelli, F., Bemporad, A., & Morari, M. (2017). 'Predictive Control for Linear and Hybrid Systems'. Cambridge: Cambridge University Press. doi:10.1017/9781139061759\n",
    "\n",
    "You should do the following: \n",
    "- Complete the function `LinearMPC.compute_finite_horizon_lqr_gain` in `linear_mpc.py`. This should return $G_0$ as detailed above\n",
    "- Hint: PyTorch and NumPy have similar APIs for linear algebra operations (by design). For instance, you can use `np.linalg.solve` or `np.linalg.inv`\n",
    "- Methods for computing $\\bar{R}$ and $\\bar{Q}$ are already provided to you\n",
    "\n",
    "**GRADING INFO**:\n",
    "\n",
    "We will be checking that the $G_{0}$ that you compute is correct for different horizons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4K8Xlp5cVBV0"
   },
   "outputs": [],
   "source": [
    "controller = LinearMPC(A, B, Q, R, horizon=20)\n",
    "G = controller.compute_finite_horizon_lqr_gain()\n",
    "print(f'Gains for horizon of {controller.H} are:')\n",
    "print(G)\n",
    "\n",
    "\n",
    "## Let's use these gains to try and stabilize the cartpole \n",
    "\n",
    "env = CartpoleEnv()\n",
    "env.reset(0.1 * np.random.randn(4))\n",
    "frames = []\n",
    "for i in tqdm(range(100)):\n",
    "    state = env.get_state()\n",
    "    control = G @ state\n",
    "    s = env.step(control)\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "    \n",
    "write_apng(\"cartpole_finite_horizon_lqr.png\", frames, delay=10)\n",
    "Image(filename=\"cartpole_finite_horizon_lqr.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 b) Infinite horizon LQR (10 points)\n",
    "\n",
    "For this question you will compute the infinite horizon LQR gain matrix. This matrix is $G$ in the lecture slides. We will denote this matrix $G_{inf}$ in this question, to show that it is the solution to the infinite-horizon case. \n",
    "\n",
    "- Complete the function `LinearMPC.compute_lqr_gain` in `linear_mpc.py`. \n",
    "- You are given the solution to the Ricatti equations which computes $\\theta^T \\theta$ directly, where $\\theta$ is the matrix defined in the lecture slides. This is termed `theta_T_theta` in the code. \n",
    "- You should find that when running the code below, the finite horizon gain for a horizon of 50 is close to the infinite horizon gain\n",
    "\n",
    "**GRADING INFO**:\n",
    "\n",
    "We will be checking that the $G_{inf}$ that you compute is correct. We will check the same value you compute in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for horizon in [5, 10, 50]:\n",
    "    controller = LinearMPC(A, B, Q, R, horizon=horizon)\n",
    "    G = controller.compute_finite_horizon_lqr_gain()\n",
    "    print(f'Gains for horizon of {controller.H} are:')\n",
    "    print(G)\n",
    "    print('')\n",
    "\n",
    "\n",
    "Ginf = controller.compute_lqr_gain()\n",
    "print(f'Gains for infinite horizon are:')\n",
    "print(Ginf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle the more we increase the horizon, the closer these should become. However, note what happens when we increase the horizon quite a bit further.\n",
    "\n",
    "The reason for this is because of the terms $A^H$ and $A^{H-1}B$ in the matrices $S$ and $M$. As we increase the horizon very far, numerical issues can occur in the computation of these large matrix exponentials. This will motivate our use of the collocation approach later in the homework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = LinearMPC(A, B, Q, R, horizon=1000)\n",
    "G = controller.compute_finite_horizon_lqr_gain()\n",
    "print('Gains for horizon of 1000 are:')\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 c) Finite horizon LQR with input constraints (20 points)\n",
    "\n",
    "In this section we will consider input constraints on the control. \n",
    "$$ \\min_{\\mathbf{u}_0, ... \\mathbf{u}_{H-1}} \\sum^H_{t=1} \\mathbf{x_t}^T Q \\mathbf{x_t} + \\mathbf{u_{t-1}}^T R \\mathbf{u_{t-1}}$$\n",
    "\n",
    "$$ \\text{subject to}$$\n",
    "\n",
    "$$ \\mathbf{x} _{t+1} = A \\mathbf{x}_t + B \\mathbf{u}_t $$\n",
    "$$ u_{min} \\leq \\mathbf{u}_t \\leq u_{max}$$\n",
    "\n",
    "Note that as mentioned in the lecture slides, there is no analytic solution to this problem in general. This means unlike for the unconstrained case, we cannot solve this for an infinite horizon. We must compute a solution for a finite horizon using optimization techniques. In addition, we cannot compute a closed-form solution as a function of $x_0$. This means that must solve this optimization at every timestep if we want to perform MPC.\n",
    "\n",
    "**GRADING INFO**:\n",
    "\n",
    "The problems you will be solving in this section are quadratic programs with unique global solutions. To grade your answer we will be solving the above optimization problem with different horizons and initial states and checking your answer against the correct global minima. \n",
    "\n",
    "#### Part i) Shooting approach (10 points)\n",
    "\n",
    "Using the definitions of $U, S, M, \\bar{Q}, \\bar{R}$ that we used for the finite horizon LQR problem, we can rewrite the objective as:\n",
    "\n",
    "$$ \\min_\\mathbf{U} (S\\mathbf{U} + M\\mathbf{x}_0)^T \\bar{Q}(S\\mathbf{U} + M\\mathbf{x}_0) + \\mathbf{U}^T \\bar{R} \\mathbf{U}$$\n",
    "\n",
    "$$ \\text{subject to}$$\n",
    "\n",
    "$$ U_{min} \\leq U \\leq U_{max} \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\: U_{max} = \\begin{bmatrix}\n",
    "u_{max}\\\\\n",
    "u_{max}\\\\\n",
    "\\vdots \\\\\n",
    "u_{max}\\\\\n",
    "\\end{bmatrix} \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\n",
    "U_{min} = \\begin{bmatrix}\n",
    "u_{min}\\\\\n",
    "u_{min}\\\\\n",
    "\\vdots \\\\\n",
    "u_{min}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can formulate this problem as a **Quadratic Program**. In this problem you will formulate this as a quadratic program and solve using `cvxpy`, a convex optimization library for Python. \n",
    "\n",
    "A useful guide on using `cvxpy` for quadratic programs can be found [here](https://www.cvxpy.org/examples/basic/quadratic_program.html)\n",
    "\n",
    "- Complete the function `LinearMPC.lqr_box_constraints_qp_shooting` in `linear_mpc.py`\n",
    "- Your function should return the entire sequence of controls $U$ rather than a single one\n",
    "- HINTS:\n",
    "    - You will need to setup the relevant decision variables with `cp.Variable`\n",
    "    - Once you have solved an optimization problem, to get the resulting `np.array` you can use the `.value` attribute of the `cp.Variable`\n",
    "    - You can easily express the above cost as `cp.quad_form(S @ U + M @ x_0, Q_bar) + cp.quad_form(U, R_bar)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will test this controller\n",
    "controller = LinearMPC(A, B, Q, R, horizon=20)\n",
    "u_min = np.array([-1])\n",
    "u_max = np.array([1])\n",
    "\n",
    "\n",
    "env = CartpoleEnv()\n",
    "env.reset(0.1 * np.random.randn(4))\n",
    "frames = []\n",
    "for i in tqdm(range(100)):\n",
    "    state = env.get_state()\n",
    "    U = controller.lqr_box_constraints_qp_shooting(state, u_min, u_max)\n",
    "    # we will only take the first action of the sequence\n",
    "    u = U[0]\n",
    "    s = env.step(u)\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "    \n",
    "write_apng(\"cartpole_box_constrained_lqr_shooting.png\", frames, delay=10)\n",
    "Image(filename=\"cartpole_box_constrained_lqr_shooting.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ii) Collocation approach (10 points)\n",
    "\n",
    "So far we have completely removed $\\mathbf{x}$ from the optimization. However using a quadratic program solver, we can use a collocation approach as discussed in the lecture slides. Here we include $\\mathbf{x}$ as a decision variable. \n",
    "\n",
    "The optimization problem is thus: \n",
    "\n",
    "$$ \\min_{\\mathbf{u}_0, ... \\mathbf{u}_{H-1}, \\mathbf{x}_{1}, ... \\mathbf{x}_{H}} \\sum^H_{t=1} \\mathbf{x_t}^T Q \\mathbf{x_t} + \\mathbf{u_{t-1}}^T R \\mathbf{u_{t-1}}$$\n",
    "\n",
    "$$\\text{subject to}$$ \n",
    "\n",
    "$$ \\mathbf{x} _{t+1} = A \\mathbf{x}_t + B \\mathbf{u}_t $$\n",
    "$$ u_{min} \\leq \\mathbf{u}_t \\leq u_{max}$$\n",
    "\n",
    "- Complete the function `LinearMPC.lqr_box_constraints_qp_collocation` in `linear_mpc.py`\n",
    "- Your function should return the entire sequence of controls $U$ and the entire sequence of states $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will test this controller\n",
    "controller = LinearMPC(A, B, Q, R, horizon=20)\n",
    "u_min = np.array([-1])\n",
    "u_max = np.array([1])\n",
    "\n",
    "\n",
    "env = CartpoleEnv()\n",
    "env.reset(0.1 * np.random.randn(4))\n",
    "frames = []\n",
    "for i in tqdm(range(100)):\n",
    "    state = env.get_state()\n",
    "    U, X = controller.lqr_box_constraints_qp_collocation(state, u_min, u_max)\n",
    "    # we will only take the first action of the sequence\n",
    "    u = U[0]\n",
    "    s = env.step(u)\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "\n",
    "write_apng(\"cartpole_box_constrained_lqr_collocation.png\", frames, delay=10)\n",
    "Image(filename=\"cartpole_box_constrained_lqr_collocation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocation vs Shooting\n",
    "\n",
    "Earlier we discussed how numerical issues could negatively affect the $S$ and $M$ matrices for large horizons. The collocation approach avoids this issue by not computing matrix exponentials.\n",
    "\n",
    "\n",
    "Now let's see what happens when we try to solve a long horizon problem. You should notice that the shooting approach fails, whereas the collocation approach succeeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = LinearMPC(A, B, Q, R, horizon=200)\n",
    "u_min = np.array([-1])\n",
    "u_max = np.array([1])\n",
    "\n",
    "# take an example start state\n",
    "state = np.array([ 0.01, -0.0005, -0.001,  0.0004])\n",
    "\n",
    "try:\n",
    "    U_shooting = controller.lqr_box_constraints_qp_shooting(state, u_min, u_max)\n",
    "    print('Shooting method solved')\n",
    "except Exception as e:\n",
    "    print('Shooting method failed to solve')\n",
    "    print(e)\n",
    "\n",
    "print('')\n",
    "\n",
    "try:\n",
    "    U_collocation, X_collocation = controller.lqr_box_constraints_qp_collocation(state, u_min, u_max)\n",
    "    print('Collocation method solved')\n",
    "except Exception as e:\n",
    "    print('Collocation method failed to solve')\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOngrpDQVCNx"
   },
   "source": [
    "# 2 - Advanced Trajectory Optimization - MPPI (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will implement Model Predictive Path Integral (MPPI) [Williams et al., ICRA 2017](https://ieeexplore.ieee.org/document/7989202), a trajectory optimization algorithm. This method uses a dynamics model (a simulator, a learned model, or analytical model) to roll out trajectories. This method has been shown to be more efficient and useful for high-dimensional spaces than other model predictive control (MPC) approaches.\n",
    "\n",
    "You can refer to the original paper for more details about the algorithm.\n",
    "\n",
    "\n",
    "In this section we are providing you a skeleton of a MPPI controller in `mppi_control.py`. You will have to implement parts of `MPPIController` that perform some of the controller key steps.\n",
    "\n",
    "**TODO**:\n",
    "Implement the following functions from `MPPIController`:\n",
    "\n",
    "1. `_rollout_dynamics`: given an initial state and a set of perturbed actions, compute the resultant states.\n",
    "\n",
    "  Compute $\\mathbf x_0, \\dots, \\mathbf x_{T}$ for each of the $K$ trajectories, where\n",
    "$$\n",
    "\\mathbf x^k_t \\leftarrow \\mathbf F(\\mathbf x^k_{t-1}, \\mathbf u_{t-1} + \\epsilon^k_{t-1})\n",
    "$$\n",
    "  where $\\epsilon_t^k$ are the perturbation applied to the nominal action sequence $\\mathbf u_0, \\dots \\mathbf u_{T-1}$ (actions without perturbations).\n",
    "    \n",
    "    You will have to use batched operations.\n",
    "2. `_compute_trajectory_cost`: given K different trajectories and control perturbations $\\epsilon$, it computes the cost associated to each of the K trajectories. For the state cost, here we will be using a quadratic state cost with *NO terminal cost*, i.e. \n",
    "$$\n",
    "\\text{cost}^k = \\sum_{t=1}^T (\\mathbf x_t^k -\\mathbf x_\\text{goal})^\\top \\mathbf Q (\\mathbf x_t^k -\\mathbf x_\\text{goal}) + \\lambda \\mathbf u_{t-1}^\\top \\Sigma^{-1} \\epsilon^k_{t-1}\n",
    "$$\n",
    "    Again, you will have to do batched operations. You should avoid using any for loop in your implementation for this part.\n",
    "3. `_nominal_trajectory_update`: Last, once we have the trajectory costs computed this part will update the nominal trajectory. In particular it will do the following operations.\n",
    "    1. $\\beta \\leftarrow \\min_{k}[\\text{cost}^k]$\n",
    "    2. $ \\gamma^k \\leftarrow  \\exp(-\\frac{1}{\\lambda}(\\text{cost}^k - \\beta))$\n",
    "    3. $\\eta \\leftarrow \\sum_{k=1}^{K} \\gamma^k$\n",
    "    4. $\\omega^k \\leftarrow \\frac{1}{\\eta}\\gamma^k$\n",
    "    5. $u_t \\mathrel{+}= \\sum_{k=1}^K \\omega^k \\epsilon^k_t$\n",
    "     Again, you will have to do batched operations. You should avoid using any for loop in your implementation for this part.\n",
    "     \n",
    "Finally you will have to play with the MPPI hyperparams for each of the two environments that we will test: the cartpole and a panda robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwtPZoS5VGnF"
   },
   "outputs": [],
   "source": [
    "from mppi_control import MPPIController, get_cartpole_mppi_hyperparams, get_panda_mppi_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - MPPI Cartpole Control (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by testing the MPPI implementation on the cartpole environment. Here, we will have the controller swing up the pole starting at a down position. Cartpole swing up requires nonlinear trajectory optimizer rather than the linear MPC we saw before. Observe that unlike the previous linear control methods, MPPI is able to swing the pole up from a down configuration.\n",
    "\n",
    "Run the following cell to test your MPPI implementation. It will perform 100 steps and then render an animated png.\n",
    "\n",
    "\n",
    "**TODO**: Modify the mppi hyperparams in `get_cartpole_mppi_hyperparams`\n",
    "\n",
    "Here there are some tips about how to tune the hyperparameters.\n",
    "1. `sigma_noise` is the covariance used to sample the control perturbations. Larger values mean you will sample larger perturbations. If you see that the robot is hardly moving you may need to increase the `sigma_noise` values to have MPPI attempt larger control values. In contrast if the robot is taking actions that are too large, you may want to decrease this value. \n",
    "2. `lambda` controls how 'peaked' the weights are around the lowest cost trajectory. As you decrease lambda, you will perform less averaging across perturbations and tend towards selecting the lowest cost perturbation. Generally lower values work better.\n",
    "3. `Q` weighs the relative importance of each state towards the cost computation. In general you can assume independence between state dimensions (i.e. `Q` diagonal). You can try weighting the position states more than the velocities, but too low weigth on velocities may result in your robot moving abruptly and having a hard time to reach the goal.\n",
    "\n",
    "**GRADING INFO**:\n",
    "Your code will be required to reach the desired configuration within an admisible error. You will have 5 different attempts to achieve the hidden goal configuration. You will be required to reach the goal at least one time. This hidden goal configuration will be similar to the one we provide for testing on the code below.\n",
    "Moreover, we will be testing your 3 implemented functions from `MPPIController` for this environment and checking that they work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 7353,
     "status": "ok",
     "timestamp": 1674590838226,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "catMXosu0P8G",
    "outputId": "68634624-16a0-442a-83af-0e7cda584a66"
   },
   "outputs": [],
   "source": [
    "env = CartpoleEnv()\n",
    "env.reset(np.array([0, np.pi, 0, 0]) + np.random.rand(4,)) \n",
    "goal_state = np.zeros(4)\n",
    "controller = MPPIController(env, num_samples=500, horizon=30, hyperparams=get_cartpole_mppi_hyperparams())\n",
    "controller.goal_state = torch.tensor(goal_state, dtype=torch.float32)\n",
    "frames = []\n",
    "num_steps = 150\n",
    "pbar = tqdm(range(num_steps))\n",
    "for i in pbar:\n",
    "    state = env.get_state()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    control = controller.command(state)\n",
    "    s = env.step(control) \n",
    "    error_i = np.linalg.norm(s-goal_state[:7])\n",
    "    pbar.set_description(f'Goal Error: {error_i:.4f}')\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "    if error_i < .2:\n",
    "        break\n",
    "print(\"creating animated gif, please wait about 10 seconds\")\n",
    "%time write_apng(\"cartpole_mppi.gif\", frames, delay=10)\n",
    "%time Image(filename=\"cartpole_mppi.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzanVmthVHHx"
   },
   "source": [
    "## 2.2 - MPPI Manipulator Control (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we will use our control implementations from the previous parts to control a higher dimensional system such a Franka Emika Panda robot arm.\n",
    "\n",
    "The Panda robot arm has 7 degrees of freedom. We will be controlling the arm via torque control, i.e. setting the torques applied to each of the joint motors directly in order to control the robot state.\n",
    "\n",
    "$$\n",
    "\\mathbf u = \\begin{bmatrix}\\tau_1, \\dots, \\tau_7\\end{bmatrix}^\\top \\in \\mathbb R^{7}\n",
    "$$\n",
    "\n",
    "In this case, our robot state is composed by each of the 7 joint angles as well as their angular velocities.\n",
    "Therefore the robot stat is:\n",
    "$$\n",
    "\\mathbf x = \\begin{bmatrix}\\theta_1, \\dots, \\theta_7, \\dot{\\theta}_1, \\dots, \\dot{\\theta}_7\\end{bmatrix}^\\top \\in \\mathbb R^{14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1056,
     "status": "ok",
     "timestamp": 1674775472322,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "TPSA_c-1VON3"
   },
   "outputs": [],
   "source": [
    "from panda_env import PandaEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our panda robot. Run the following cell to visualize the robot dynamics with a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "executionInfo": {
     "elapsed": 29917,
     "status": "ok",
     "timestamp": 1674594598879,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "jyWMl9pieMsR",
    "outputId": "bf19835c-5f40-4114-8c72-8cc65a67542b"
   },
   "outputs": [],
   "source": [
    "# Demo with random actions:\n",
    "env = PandaEnv()\n",
    "env.reset()\n",
    "frames=[] #frames to create animated png\n",
    "frames.append(env.render())\n",
    "for i in range(100):\n",
    "    action = 13*np.random.randn(7) # Random actions\n",
    "    s = env.step(action)\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "print(\"creating animated gif, please wait about 10 seconds\")\n",
    "%time write_apng(\"panda_demo.gif\", frames, delay=10)\n",
    "%time Image(filename=\"panda_demo.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run MPPI on the panda environment to drive it to a desired state (visualized in green). In this part we will be using the same controller implementation as part 1.4. In this part we will show that MPPI can be applied to higher dimensional systems.\n",
    "You will not have to modify your MPPI implementation, but you will have to tune with the hyperparameters. As in the cartpole case you will have to complete the function `get_panda_mppi_hyperparams` in `mppi_control.py` that returns the correct hyperparameters.\n",
    "\n",
    "**TODO**: Modify the mppi hyperparams in `get_panda_mppi_hyperparams`\n",
    "\n",
    "In green we show the desired robot state. \n",
    "\n",
    "**GRADING INFO**:\n",
    "Your code will be required to reach the desired configuration within an admisible error. You will have 5 different attempts to achieve the hidden goal configuration. You will be required to reach the goal at least one time. This hidden goal configuration will be similar to the one we provide for testing on the code below. It will always start from the zero state, i.e. $\\mathbf x_0 = \\begin{bmatrix}0 & \\dots & 0 \\end{bmatrix}^\\top$.\n",
    "\n",
    "Moreover, we will be testing your 3 implemented functions from `MPPIController` for this environment and checking that they work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "executionInfo": {
     "elapsed": 28961,
     "status": "ok",
     "timestamp": 1674595062850,
     "user": {
      "displayName": "Miquel Oller Oliveras",
      "userId": "17926929657203297355"
     },
     "user_tz": 300
    },
    "id": "MwCz8cQTpFKE",
    "outputId": "3ce4b3a3-73f7-4405-e824-b885f6ce580e"
   },
   "outputs": [],
   "source": [
    "# MPPI Controlled \n",
    "env = PandaEnv()\n",
    "env.reset()\n",
    "env.set_state(np.zeros(14))\n",
    "controller = MPPIController(env, num_samples=100, horizon=50, hyperparams=get_panda_mppi_hyperparams())\n",
    "controller.goal_state[:7] = torch.tensor([0.07592427029735163, -0.8475015226665294, -0.03853894349730421, -2.568912556631523, -0.046517021175887845, 2.0, 0.8269477256149668])\n",
    "goal_state = controller.goal_state.detach().cpu().numpy()\n",
    "env.goal_state = goal_state\n",
    "\n",
    "frames = []\n",
    "states = []\n",
    "\n",
    "num_steps = 200\n",
    "pbar = tqdm(range(num_steps))\n",
    "for i in pbar:\n",
    "    state = env.get_state()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    control = controller.command(state)\n",
    "    s = env.step(control)\n",
    "    states.append(s)\n",
    "    error_i = np.linalg.norm(s[:7]-goal_state[:7]) # only consider the joint positions to compute the score\n",
    "    pbar.set_description(f'Goal Error: {error_i:.4f}')\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "    if error_i < 1.2:\n",
    "        break\n",
    "\n",
    "\n",
    "print('Achieved State: ', env.get_state())\n",
    "print('Goal State: ', controller.goal_state)\n",
    "\n",
    "print(\"creating animated gif, please wait about 10 seconds\")\n",
    "\n",
    "%time write_apng(\"panda_mppi.gif\", frames, delay=10)\n",
    "%time Image(filename=\"panda_mppi.gif\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRQ/BgPmM29eFX3MyfIYKz",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
