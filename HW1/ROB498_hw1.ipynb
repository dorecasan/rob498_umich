{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV2tDhNAxBXi"
   },
   "source": [
    "# ROB 498: Robot Learning for Planning and Control\n",
    "# Assignment 1: Introduction to PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PhImJ85xix9"
   },
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKlU12PTzIZn"
   },
   "outputs": [],
   "source": [
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a ROB498 folder and put all the files under A1 folder, then 'ROB498/HW1'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ROB498/HW1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcYCH3lWzKto"
   },
   "source": [
    "### Setup Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WR9nHjCHxAFE"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d_01Y1yy4E7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQdugHRazlsC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "\n",
    "files = os.listdir(GOOGLE_DRIVE_PATH)\n",
    "expected_files = [ 'ROB498_hw1.ipynb', 'pytorch_intro.py', 'hands_on_regression.py', 'regression_training_data.npz', 'regression_validation_data.npz',  'robot_kinematics_regression.py', 'robot_kinematics_training_data.npz', 'robot_kinematics_validation_data.npz',  ]\n",
    "print(f'Files found: {files}')\n",
    "\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "# Verify that there are all the expected files in the directory\n",
    "all_found = True\n",
    "for expected_file in expected_files:\n",
    "  if expected_file not in files:\n",
    "    print(f'Required file {expected_file} not found!')\n",
    "    all_found = False\n",
    "if all_found:\n",
    "  print('All required files are found :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr3mO5nTzyBR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77GR7M460IkA"
   },
   "source": [
    "## Assignment Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due 2/1 at 11:59pm\n",
    "\n",
    "**Rules**:\n",
    "\n",
    "1. All homework must be done individually, but you are encouraged to post questions on Piazza\n",
    "\n",
    "2. No late homework will be accepted (unless you use your late-day tokens)\n",
    "\n",
    "3. Submit your code on [autograder.io](http://autograder.io/)\n",
    "\n",
    "4. Remember that copying-and-pasting code from other sources is not allowed\n",
    "\n",
    "5. The use of additional package imports beyound the packages we provide is not allowed. The autograder will not grade your code if you use additional packages."
   ],
   "metadata": {
    "id": "11n8VqQ6B79C"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWDwT0WLxqfN"
   },
   "source": [
    "## 1 - PyTorch Basics (24 points)\n",
    "\n",
    "In this section you will practice some basic tensor operations in PyTorch. Some of these questions were adapted from Juston Johnson's Deep Learning for Computer Vision class (https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/assignment1.html.) \n",
    "\n",
    "\n",
    "**GRADING GUIDE**\n",
    "\n",
    "Each part will be evaluated on 2 test (1 point each) making each part worth 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBz8MNRexz6M"
   },
   "outputs": [],
   "source": [
    "# Imports:\n",
    "from pytorch_intro import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "e4Ag2TTtDON8"
   },
   "source": [
    "### Tensor Basics\n",
    "\n",
    "a) Complete the function `create_tensor_of_pi` in `pytorch_intro.py`. The function takes as arguments integers M and N and should return a tensor of shape (M, N) which is filled with the value of 3.14\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40FlTZEYx66Z"
   },
   "outputs": [],
   "source": [
    "M, N = 3, 2\n",
    "\n",
    "pi = create_tensor_of_pi(M, N)\n",
    "print('Here is the tensor')\n",
    "print(pi)\n",
    "\n",
    "print(f'Shape of tensor: {pi.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aAMD3oqnDON9"
   },
   "source": [
    "b) Slice indexing - fill in the function `slice_indexing_practice` in `pytorch_intro.py`. The function takes as input a tensor of shape (M, N). The function should return the following:\n",
    "- The last row of x. This should be shape (N,)\n",
    "- The third column of x. This should be of shape (M, 1)\n",
    "- Tensor of shape (2, 3) which contains the first two rows and first three columns of x\n",
    "- even_rows_odd_cols: Two dimensional tensor containing the even rows and odd columns of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHye0LqyDON9"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10],\n",
    "    [11, 12, 13, 14, 15],\n",
    "    [16, 17, 18, 19, 20]\n",
    "])\n",
    "\n",
    "(last_row, third_col, first_two_rows_three_cols, even_rows_odd_cols) = slice_indexing_practice(x)\n",
    "\n",
    "print('Last row is ')\n",
    "print(last_row)\n",
    "print('Third col is')\n",
    "print(third_col)\n",
    "print('First two rows and first three columns are')\n",
    "print(first_two_rows_three_cols)\n",
    "print('Even rows and odd columns')\n",
    "print(even_rows_odd_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "p4GcchzcDON-"
   },
   "source": [
    "c) Slice assigment - fill in the function `slice_assignment_practice` in `pytorch_intro.py`. The function takes as input a two-dimensional tensor of shape (M, N). You should modify the input such that the first 4 rows and 6 columns are equal to:\n",
    "\n",
    "    [0 1 2 2 2 2]\n",
    "    [0 1 2 2 2 2]\n",
    "    [3 4 3 4 5 5]\n",
    "    [3 4 3 4 5 5]\n",
    "\n",
    "The rest of the tensor should remain unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFdbIUUgDON-"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(6, 8)\n",
    "print('x is ')\n",
    "print(x)\n",
    "\n",
    "print('modified x is')\n",
    "print(slice_assignment_practice(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ppbgoFTrDON-"
   },
   "source": [
    "d) Shuffle columns - complete the function `shuffle_cols`. This function takes as input a tensor of shape (M, N). It should return the following tensor y:\n",
    "- The first two columns of y are copies of the first column of x\n",
    "- The third column of y is the same as the third column of x\n",
    "- The fourth column of y is the same as the second column of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwjrP9woDON-"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[-2, -4, -1, 1],\n",
    "                  [-5, -3, 5, -3],\n",
    "                  [4, 4, -5, -3],\n",
    "                  [2, -5, -2, -5],\n",
    "                  [-1, -3, 0, 1],\n",
    "                  [-6, 4, 2, -1]])\n",
    "y = shuffle_cols(x)\n",
    "print('y is ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "oxOssL9aDON_"
   },
   "source": [
    "e) Reverse rows - fill in the function `reverse_rows.py`. This function takes as input a tensor of shape (M, N). It should return a tensor with the rows reversed. i.e. the first row should be equal to the last row of x, the second row should be equal to the second to last row of x, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z54GLi6jDON_"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 1, 1, 1],\n",
    "                  [2, 2, 2, 2],\n",
    "                  [3, 3, 3, 3],\n",
    "                  [4, 4, 4, 4],\n",
    "                  [5, 5, 5, 5]])\n",
    "print('Reversed row of x is')\n",
    "print(reverse_rows(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0wlGMR73DON_"
   },
   "source": [
    "f) Reshaping - Complete the function `reshape_practice`. The function takes a tensor of shape (24,). It should return a tensor of shape (3, 8) such that:\n",
    "\n",
    "    y = [[x[0], x[1], x[2],  x[3],  x[12], x[13], x[14], x[15]],\n",
    "         [x[4], x[5], x[6],  x[7],  x[16], x[17], x[18], x[19]],\n",
    "         [x[8], x[9], x[10], x[11], x[20], x[21], x[22], x[23]]]\n",
    "\n",
    "   You should complete this function by using `torch.reshape`, `torch.transpose` and `torch.permute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDptaYceDON_"
   },
   "outputs": [],
   "source": [
    "x = torch.arange(0, 24)\n",
    "print('x is ')\n",
    "print(x)\n",
    "\n",
    "print('x reshaped is')\n",
    "print(reshape_practice(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YNnY7oRfDOOA"
   },
   "source": [
    "g) Batched matrix multiplication - you should complete the function `batched_matrix_multiply`. This function takes two matrices x and y. x is of shape (B, N, M) and y is of shape (B, M, P). B is the batched dimension. The function should return the result of the B matrix multiplications, which should be of shape (B, N, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rcUe2pwDOOA"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[[3, 0, 0],\n",
    "                   [-1, -1, -4],\n",
    "                   [-2, -4, 1]],\n",
    "\n",
    "                  [[-1, 2, 1],\n",
    "                   [1, -2, -3],\n",
    "                   [3, 0, -1]]])\n",
    "y = torch.tensor([[[0, 1],\n",
    "                   [-4, 0],\n",
    "                   [-3, 0]],\n",
    "\n",
    "                  [[-2, -4],\n",
    "                   [0, 1],\n",
    "                   [-3, 1]]])\n",
    "\n",
    "print('The first matrix multiplication is')\n",
    "print(torch.mm(x[0], y[0]))\n",
    "\n",
    "print('The second matrix multiplcation is')\n",
    "print(torch.mm(x[1], y[1]))\n",
    "\n",
    "print('The batched matrix multiplications are')\n",
    "print(batched_matrix_multiply(x, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "U24PjtmIDOOA"
   },
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FER7uWgHDOOA"
   },
   "source": [
    "h) Scalar function and gradient - complete the function `compute_scalar_function_and_grad`. The function should compute the function $y = 3 x^2$ and compute the gradient using PyTorch autograd. You should use .backward() to compute the gradient, and the function should only return y. If you have successfully implemented the function, the gradient of x should be stored in x.grad after the function has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESn1X3rkDOOA"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1.5])\n",
    "\n",
    "print('y is')\n",
    "print(compute_scalar_function_and_grad(x).item())\n",
    "\n",
    "print('Gradient of x is')\n",
    "print(x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "POPwKY6CDOOB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sGpksIpADOOB"
   },
   "source": [
    "i) Vector function and gradient. Complete the function `compute_vector_function_and_grad`. This function should compute the vector function $y = \\begin{bmatrix} cos(2 x_1 + x_2) \\\\ sin(2 x_2 - x_1) \\end{bmatrix}$ from input $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$. You should use autograd to compute the quantity $\\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} + \\frac{\\partial y_2}{\\partial x_1} \\\\ \\frac{\\partial y_1}{\\partial x_2} + \\frac{\\partial y_2}{\\partial x_2} \\end{bmatrix}$. If you do this successfully the result should be stored in x.grad. This is known as a Jacobian-vector product and is the product $\\frac{dy}{dx} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEoKkKu_DOOB"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([-0.5, 0.5])\n",
    "y = compute_vector_function_and_grad(x)\n",
    "\n",
    "print('y is ')\n",
    "print(y)\n",
    "\n",
    "print('the Jacobian-vector product is')\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5LZYYovSDOOB"
   },
   "source": [
    "j) Scalar function and partial gradient - you should complete the function `compute_scalar_function_and_partial_grad`. This function takes as input two tensors of shape (1) x and y, and should return $z=y\\sqrt(x)$. You should use pytorch autograd to compute the gradient of z with respect to x, but not with y. If this is implemented correctly the gradient with respect to x should be stored in `x.grad` and `y.grad` should be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ey-Hc3NbDOOB"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([4.])\n",
    "y = torch.tensor([0.5])\n",
    "\n",
    "print('z is ')\n",
    "print(compute_scalar_function_and_partial_grad(x, y))\n",
    "print('grad x is ')\n",
    "print(x.grad)\n",
    "print('grad y is ')\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Forward Kinematics"
   ],
   "metadata": {
    "id": "lpQPxKoBH9-U"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNUWKwyjDULe"
   },
   "source": [
    "k) \n",
    "In this part, we will compute the manipulator Jacobian for a 2-link planar robot.\n",
    "\n",
    "In robotics, the Jacobian relates the velocities in joint space to the end-effector velocities. \n",
    "\n",
    "Given the robot joints values $\\mathbf\\theta = \\begin{bmatrix}\\theta_1 &\\dots & \\theta_n\\end{bmatrix}^\\top\\in \\mathbb R^n$, the forward kinematics function $f_\\text{FK}$ maps the joint values to the end-effector coordinates $\\mathbf x\\in \\mathbb R^m$. Therefore, the forward kinematics can be written as:\n",
    "$$\n",
    "\\mathbf x = f_\\text{FK}(\\mathbf \\theta)\n",
    "$$\n",
    "To obtain the end-effector velocities $\\dot{\\mathbf x} = \\frac{d\\mathbf x}{dt}\\in\\mathbb R^m$, by the chain rule it can be computed as\n",
    "$$\n",
    "\\dot{\\mathbf x} = \\frac{\\partial f_\\text{FK}(\\mathbf \\theta)}{\\partial \\theta}\\frac{d\\theta(t)}{dt} = \\frac{\\partial f_\\text{FK}(\\mathbf \\theta)}{\\partial \\theta}\\dot{\\theta} = \\mathbf J(\\theta) \\dot{\\theta}\n",
    "$$\n",
    "where $\\mathbf J(\\theta)\\in \\mathbb R^{m\\times n}$ is called the **Jacobian**.\n",
    "The Jacobian matrix represents the linear sensitivity of the end-effector velocity $\\dot{\\mathbf x}$ to the joint velocity $\\dot{\\mathbf \\theta}$ as a function of the joint variables $\\mathbf \\theta$.\n",
    "\n",
    "> For more information: [Lynch, Kevin. Modern Robotics.](https://modernrobotics.northwestern.edu/nu-gm-book-resource/5-1-1-space-jacobian/)\n",
    "\n",
    "In this section, we will compute the Jacobina using the torch `autograd` tools.\n",
    "\n",
    "Given a 2-link robot with link length $L_1$ and $L_2$, and the robot joints $\n",
    "\\theta = \\begin{bmatrix}\\theta_1 & \\theta_2\\end{bmatrix}^\\top$, the forward kinematics are given by\n",
    "$$\n",
    "\\mathbf x = \\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix} = f_\\text{FK}(\\theta) = \\begin{bmatrix} L_1 \\cos\\theta_1 + L_2\\cos(\\theta_1 + \\theta_2) \\\\ L_1\\sin\\theta_1 + L_2\\sin(\\theta_1 + \\theta_2)\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![2-link manipulator](https://drive.google.com/uc?export=view&id=10gWhX8Eyfx4YFkfNYpGWenIdfgtF0Re5)\n",
    "\n",
    "\n",
    "TODO: \n",
    " 1. Implement `compute_forward_kinematics` in `pytorch_intro.py`.\n",
    " 2. Implement `compute_jacobian` in `pytorch_intro.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3zvEGsjwi9S"
   },
   "outputs": [],
   "source": [
    "# Compute Forward Kinematics:\n",
    "\n",
    "thetas = torch.zeros(2, requires_grad=True)\n",
    "print(f'Forward kinematics -- thetas {thetas} end-effector position: {compute_forward_kinematics(thetas)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4pxdo40qpoy"
   },
   "outputs": [],
   "source": [
    "# Compute Jacobian:\n",
    "\n",
    "print(f'The jacobian for the configuration thetas={thetas} is:\\n{compute_jacobian(thetas)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R9c0Qh5x7Tv"
   },
   "source": [
    "## 2 - Hands on Regression (46 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ucf5jJkmWBim"
   },
   "source": [
    "In this second part, we will review some classical regression methods and compare them with Neural Networks. We will also cover some data handling basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCqbRKmoiBHC"
   },
   "outputs": [],
   "source": [
    "from hands_on_regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekp7NtLHae2C"
   },
   "source": [
    "### Load Regression Data\n",
    "\n",
    "We will work with some data, and the goal is to find a function that approximates them. Here, we first load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26qvcNLdUX5b"
   },
   "outputs": [],
   "source": [
    "train_data_path = os.path.join(GOOGLE_DRIVE_PATH, 'regression_training_data.npz')\n",
    "val_data_path = os.path.join(GOOGLE_DRIVE_PATH, 'regression_validation_data.npz')\n",
    "\n",
    "train_data = np.load(train_data_path)\n",
    "val_data = np.load(val_data_path)\n",
    "\n",
    "# Unpack the data\n",
    "xs = torch.from_numpy(train_data['x'])[:, None] # shape (N, 1)\n",
    "ys = torch.from_numpy(train_data['y'])[:, None] # shape (N, 1)\n",
    "\n",
    "xs_val = torch.from_numpy(val_data['x'])[:, None] # shape (N, 1)\n",
    "ys_val = torch.from_numpy(val_data['y'])[:, None] # shape (N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAQ4MdfAp9wV"
   },
   "source": [
    "Next, here we provide some code to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNj6b6Fcavzu"
   },
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "plt.scatter(xs, ys, color='b', label='train_data')\n",
    "plt.scatter(xs_val, ys_val, color='g', label='val_data')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvOG8AxwaQri"
   },
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZrK4qbu3yRH"
   },
   "source": [
    "We will start by approximating the data with a polynomia of degree $d$.\n",
    "\n",
    "$ f(x)=c_0 + c_1 x + c_2 x^2 + \\dots + c_{d}x^d$\n",
    "\n",
    "Implement `polynomial_basis_functions` in `hands_on_regression.py` (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CLYAK38adln"
   },
   "outputs": [],
   "source": [
    "degree = 5\n",
    "Xs_polynomial = polynomial_basis_functions(xs, degree)\n",
    "\n",
    "print(f'Polynomial expansion size: {Xs_polynomial.shape}. Expected size: torch.Size([{len(xs)}, {degree+1}])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFgSvhTdYlFA"
   },
   "source": [
    "#### Linear Least Squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jm5k7_h26gri"
   },
   "source": [
    "Implement `compute_least_squares_solution` in `hands_on_regression.py` to compute the Least Squares regression solution. (6 points)\n",
    "\n",
    " This should compute the coefficients $[c_0,\\dots, c_d]$ that best fit the data `ys` given the $d$-degree plynomial expanded data `Xs_polynomial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4U5-Qv1rf1wU"
   },
   "outputs": [],
   "source": [
    "coeffs_poly = compute_least_squares_solution(Xs_polynomial, ys)\n",
    "print(f'Computed coefficients: {coeffs_poly} \\n-- It should be a torch tensor of size torch.Size([{degree+1}])). Returned size: {coeffs_poly.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6WhM-H6ggQW"
   },
   "outputs": [],
   "source": [
    "# visualize the polynomial fit\n",
    "x_poly = torch.linspace(-5, 5, 1000)[:, None]\n",
    "y_poly = torch.sum(polynomial_basis_functions(x_poly, degree) * coeffs_poly, axis=1)\n",
    "\n",
    "plt.scatter(xs, ys, color='b', label='train_data')\n",
    "plt.scatter(xs_val, ys_val, color='g', label='val_data')\n",
    "plt.plot(x_poly, y_poly, color='orange', label='least squared solution')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixH6ezmu8Dnn"
   },
   "source": [
    "As you can see, the fit is not great. Way need to increse the polynomial degree.\n",
    "The following code will test multiple degrees. Which one is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25X2oKb98Jyc"
   },
   "outputs": [],
   "source": [
    "degrees = [0, 1, 2, 3, 4, 5, 6, 7, 10]  # TODO: Modify this value\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(degrees), figsize=(4 * len(degrees), 3))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    Xs_polynomial = polynomial_basis_functions(xs, degree)\n",
    "\n",
    "    coeffs_poly = compute_least_squares_solution(Xs_polynomial, ys)\n",
    "\n",
    "    x_poly = torch.linspace(-5, 5, 1000)[:, None]\n",
    "    y_poly = torch.sum(polynomial_basis_functions(x_poly, degree) * coeffs_poly, axis=1)\n",
    "\n",
    "    ax_i = axes[i]\n",
    "    ax_i.scatter(xs, ys, color='b', label='train_data')\n",
    "    ax_i.scatter(xs_val, ys_val, color='g', label='val_data')\n",
    "    ax_i.plot(x_poly, y_poly, color='orange', label='least squared solution')\n",
    "    ax_i.set_title(f'Polynomial degree: {degree}')\n",
    "    ax_i.grid()\n",
    "axes[-1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o5DwRIkaUon"
   },
   "source": [
    "#### Linear Least Squares using Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh9Dg0UtsWEa"
   },
   "source": [
    "Now, we will do the same, but with a Neural Network. \n",
    "In this section, we will regress the coefficients $[c_0, \\dots, c_d]$ using a single Linear layer a without bias.\n",
    "You will have to implement the network, as well as the train and validation pipeline.\n",
    "Here onwards, we will choose $d=6$.\n",
    "\n",
    "But first, you will have to implement some basic functions to normalize the data.\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Implement `get_normalization_constants` which given a tensors, it returns its normalization constants. The normalization constants are the mean and the std. (2 points)\n",
    "* Implement `normalize_tensor` that returns the normalized tensor for the given normalization constants. (2 points)\n",
    "* Implement `denormalize_tensor` that inverts the normalization of tensor for the given normalization constants. (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt1UrUq4iCdI"
   },
   "outputs": [],
   "source": [
    "# Prepare the data:\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, Xs, ys):\n",
    "        self.Xs = Xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_i = self.Xs[idx]\n",
    "        y_i = self.ys[idx]\n",
    "        return x_i, y_i\n",
    "\n",
    "\n",
    "\n",
    "# #xpand data using the polynomial basis fuctions\n",
    "degree = 6 # Minimum degree that explains the data.\n",
    "Xs_polynomial = polynomial_basis_functions(xs, degree)\n",
    "Xs_polynomial_val = polynomial_basis_functions(xs_val, degree)\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "X_mean, X_std = get_normalization_constants(Xs_polynomial)\n",
    "y_mean, y_std = get_normalization_constants(ys)\n",
    "Xs_polynomial_norm = normalize_tensor(Xs_polynomial, X_mean, X_std)\n",
    "ys_polynomial_norm = normalize_tensor(ys, y_mean, y_std) \n",
    "Xs_polynomial_val_norm = normalize_tensor(Xs_polynomial_val, X_mean, X_std )\n",
    "ys_polynomial_val_norm = normalize_tensor(ys_val, y_mean, y_std)\n",
    "\n",
    "# Force Xs to have the first column as 1s to have the bias effect and avoid normalization\n",
    "Xs_polynomial_norm[:,0] = 1 # Force biases to 1\n",
    "Xs_polynomial_val_norm[:, 0] = 1 # Force biases to 1\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = SimpleDataset(Xs_polynomial_norm, ys_polynomial_norm)\n",
    "val_dataset = SimpleDataset(Xs_polynomial_val_norm, ys_polynomial_val_norm)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CUERnn31Ob5"
   },
   "source": [
    "We will save the normalization constants for evaluation. You will have to submit `regression_norm_constants.pt` along with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crXw9JOw1Nz9"
   },
   "outputs": [],
   "source": [
    "# save normalization constants:\n",
    "norm_constants = {\n",
    "   'X_mean': X_mean,\n",
    "   'X_std': X_std,\n",
    "   'y_mean': y_mean,\n",
    "   'y_std': y_std, \n",
    "}\n",
    "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'regression_norm_constants.pt')\n",
    "torch.save(norm_constants, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CKP-byks7si"
   },
   "source": [
    "Next, we will train our model. \n",
    "Implement the `LinearRegressor` class in `hands_on_regression.py`. \n",
    "This NN should be a single Linear layer without biases.\n",
    "Remember to initialize the NN weights in the `__init__`. \n",
    "\n",
    "You will also have to implement the training functionalities. Explicitly, you need to implement:\n",
    "* `train_step`: Perfoms an epoch of training the model.\n",
    "* `val_step`: Perfoms an epoch of model performance validation. \n",
    "* `train_model`: Trains the given model for `num_epochs` epochs. You may need to use `train_step` and `val_step`.\n",
    "\n",
    "For loss, use MSE loss. You can use `F.mse_loss`.\n",
    "For parameter optimization, use Stochastic Gradient Descent (SGD).\n",
    "\n",
    "\n",
    "The training should take about 2 min.\n",
    "\n",
    "Check the loss curves and modify the `NUM_EPOCHS` and `LR` hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5qAh2JyZbzx"
   },
   "outputs": [],
   "source": [
    "LR = 0.05\n",
    "NUM_EPOCHS = 20000\n",
    "\n",
    "linear_regressor = LinearRegressor(Xs_polynomial.shape[-1])\n",
    "\n",
    "train_losses, val_losses = train_model(linear_regressor, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LR)\n",
    "\n",
    "# plot train loss and test loss:\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3))\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].grid()\n",
    "axes[0].set_title('Train Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Train Loss')\n",
    "axes[0].set_yscale('log')\n",
    "axes[1].plot(val_losses)\n",
    "axes[1].grid()\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNgDLZIiuXAR"
   },
   "source": [
    "We will save the trained model. You will have to submit it along your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_9xuDu5vWA2"
   },
   "outputs": [],
   "source": [
    "# save model:\n",
    "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'linear_regressor.pt')\n",
    "torch.save(linear_regressor.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpzBsG2ovWS0"
   },
   "source": [
    "Let's visualize the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUo3FH5WlDNh"
   },
   "outputs": [],
   "source": [
    "# create evaluation data:\n",
    "x_nn = torch.linspace(-5, 5, 1000, dtype=torch.float32)[:,None]\n",
    "X_nn = polynomial_basis_functions(x_nn, degree)\n",
    "# Normalize data\n",
    "X_nn_norm = normalize_tensor(X_nn, X_mean, X_std )\n",
    "X_nn_norm[:,0] = 1 # Force biases to 1\n",
    "\n",
    "y_nn_norm = linear_regressor(X_nn_norm)\n",
    "y_nn = denormalize_tensor(y_nn_norm, y_mean, y_std).detach().cpu()\n",
    "plt.scatter(xs, ys, color='b', label='train_data')\n",
    "plt.scatter(xs_val, ys_val, color='g', label='val_data')\n",
    "plt.plot(x_nn, y_nn, color='orange', label='NN regression solution')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GRADING GUIDE**\n",
    "\n",
    "Your `linear_regressor` implementation will be graded by evaluating its predictive accuracy on the validation data provided (6 points) as well as a hidden test data set (6 points).\n",
    "Your model will have to have a predicive score of less than 0.05.\n",
    "\n",
    "We will also evaluate the model to have the desired architecture (4 points)"
   ],
   "metadata": {
    "id": "xLcLeOWbpIwf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRI3KqJS27DZ"
   },
   "outputs": [],
   "source": [
    "y_pred_norm = linear_regressor(Xs_polynomial_norm)\n",
    "y_pred = denormalize_tensor(y_pred_norm, y_mean, y_std)\n",
    "train_score = F.mse_loss(y_pred, ys).item()\n",
    "y_pred_norm_val = linear_regressor(Xs_polynomial_val_norm)\n",
    "y_pred_val = denormalize_tensor(y_pred_norm_val, y_mean, y_std)\n",
    "val_score = F.mse_loss(y_pred_val, ys_val).item()\n",
    "print(f'Train set score: {train_score}')\n",
    "print(f'Validation set score: {val_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM40K-2iusCt"
   },
   "source": [
    "Observe that although the neural network can do a good job approximating the data, its performance is only good within the training distribution. \n",
    "\n",
    "Observe predicted values outside the training distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieCBBYPU4bgC"
   },
   "outputs": [],
   "source": [
    "# Visualize out-of-distribution performance:\n",
    "x_nn = torch.linspace(-10, 10, 1000, dtype=torch.float32)[:, None]\n",
    "X_nn = polynomial_basis_functions(x_nn, degree)\n",
    "# Normalize data\n",
    "X_nn_norm = normalize_tensor(X_nn, X_mean, X_std )\n",
    "X_nn_norm[:,0] = 1 # Force biases to 1\n",
    "\n",
    "y_nn_norm = linear_regressor(X_nn_norm)\n",
    "y_nn = denormalize_tensor(y_nn_norm, y_mean, y_std).cpu().detach()\n",
    "print(y_nn.shape)\n",
    "plt.scatter(xs, ys, color='b', label='train_data')\n",
    "plt.scatter(xs_val, ys_val, color='g', label='val_data')\n",
    "plt.plot(x_nn, y_nn, color='orange', label='NN regression solution')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eq-ESFBpDot"
   },
   "source": [
    "### Neural Network as a General Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__7ItqNeg2fp"
   },
   "source": [
    "In this section we will show that NN are capable of approximating general functions without the need of a set of basis functions.\n",
    "\n",
    "TODO: \n",
    "\n",
    "* Implement the `GeneralNN`, which should be a 3 fully-connected layers with hidden sizes 100 and Tanh activations. The network input features are one-dimensional as well as the output features.\n",
    "\n",
    "Note that in this case we are not normalizing the data. For this case observe that the performance of deep neural netowrks is good even without normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGofDUoDg0SA"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "LR = 1e-2\n",
    "NUM_EPOCHS = 15000\n",
    "\n",
    "# Initialize the model\n",
    "general_nn = GeneralNN()\n",
    "\n",
    "# Create dataset without polynomial basis functions:\n",
    "train_dataset = SimpleDataset(xs, ys)\n",
    "val_dataset = SimpleDataset(xs_val, ys_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_model(general_nn, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LR)\n",
    "\n",
    "# plot train loss and test loss:\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3))\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].grid()\n",
    "axes[0].set_title('Train Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Train Loss')\n",
    "axes[0].set_yscale('log')\n",
    "axes[1].plot(val_losses)\n",
    "axes[1].grid()\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdpLi-rgGjvS"
   },
   "outputs": [],
   "source": [
    "# save model:\n",
    "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'general_nn.pt')\n",
    "torch.save(general_nn.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1U18vcXHiGkr"
   },
   "outputs": [],
   "source": [
    "# Create evaluation data\n",
    "x_nn = torch.linspace(-5, 5, 1000, dtype=torch.float32)[:, None]\n",
    "X_nn = x_nn\n",
    "\n",
    "# Call the method\n",
    "y_nn = general_nn(X_nn).cpu().detach()\n",
    "\n",
    "# Plot the data to evaluate the fit.\n",
    "plt.scatter(xs, ys, color='b', label='train_data')\n",
    "plt.scatter(xs_val, ys_val, color='g', label='val_data')\n",
    "plt.plot(x_nn, y_nn, color='orange', label='general nn solution')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GRADING GUIDE**\n",
    "\n",
    "Your `general_nn` implementation will be graded by evaluating its predictive accuracy on the validation data provided (6 points) as well as a hidden test data set (6 points).\n",
    "Your model will have to have a predicive score of less than 0.01.\n",
    "\n",
    "We will also evaluate the model to have the desired architecture (4 points)"
   ],
   "metadata": {
    "id": "iEIKNv4prsuG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y_pred = general_nn(xs[:, None])\n",
    "train_score = F.mse_loss(y_pred, ys).item()\n",
    "y_pred_val = general_nn(xs_val[:, None])\n",
    "val_score = F.mse_loss(y_pred_val, ys_val).item()\n",
    "print(f'Train set score: {train_score}')\n",
    "print(f'Validation set score: {val_score}')"
   ],
   "metadata": {
    "id": "ZSEeaB47rrIV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZv0SRWsT39o"
   },
   "source": [
    "## 3 - Robot Kinematics Regression (30 points)\n",
    "\n",
    "We have already seen an example of a robot kinematics problem for a 2-link robot manipulator, here we will consider a 3-link robot manipulator. \n",
    "\n",
    "Recall that the forward kinematics function is $$\\mathbf x = f_\\text{FK}(\\mathbf \\theta)$$\n",
    "\n",
    "Unfortunately there is noise and a systematic error between the actual end-effector position $\\mathbf{x}$ and the end-effector position predicted from the analytic kinematics solution. This error is a nonlinear function of the commanded joint configuration $\\theta_{\\text{des}}$. See the image below.\n",
    "\n",
    "This error could come from a number of sources - for example the robot's motors could be unable to supply adequate torque for counteracting gravity in some configurations, or there could be state-dependent error in the robot joint encoders. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "![underpowered_3_link_manipulator](https://drive.google.com/uc?export=view&id=19TwxG36_cUcnKTQZBOwn2KNWO8zdidQd)"
   ],
   "metadata": {
    "id": "mBrTQooVT3lb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this question, you are given a dataset of the commanded joint configuration $\\theta_{des}$ and the corresponding end-effector positions $\\mathbf x$. Your task is to learn a Neural Network to approximate the function\n",
    "\n",
    " $$  \\mathbf x = f_\\text{FK}(\\mathbf \\theta_{des})$$\n",
    "\n",
    " For this problem you need to:\n",
    "\n",
    " * Complete the `MLP` class in `robot_kinematics_regression.py`\n",
    "      * The Neural Network should contain 1 hidden layer\n",
    "      * The hidden layer size should be 128\n",
    "      * You should use ReLU activations\n",
    " * Train your model on the dataset below.\n",
    "      * You should normalize the features and targets of this dataset\n",
    " * Save the trained model as `robot_kinematics_model.pt`\n",
    "\n",
    "For this question you will only be assessed on\n",
    "1. Completing the `MLP` class correctly\n",
    "2. The final performance of your learned model \n",
    "\n",
    "You are encouraged to reuse code from the previous question. \n"
   ],
   "metadata": {
    "id": "nlgAUY5LjLM2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from robot_kinematics_regression import * \n",
    "from hands_on_regression import *\n",
    "\n",
    "# Prepare the data:\n",
    "train_data_path = os.path.join(GOOGLE_DRIVE_PATH, 'robot_kinematics_training_data.npz')\n",
    "val_data_path = os.path.join(GOOGLE_DRIVE_PATH, 'robot_kinematics_validation_data.npz')\n",
    "\n",
    "train_data = np.load(train_data_path)\n",
    "val_data = np.load(val_data_path)\n",
    "\n",
    "# Unpack the data\n",
    "# we will use x for input and y for target to keep consistent with previous question\n",
    "x = torch.from_numpy(train_data['theta'])\n",
    "y = torch.from_numpy(train_data['x'])\n",
    "\n",
    "x_val = torch.from_numpy(val_data['theta'])\n",
    "y_val = torch.from_numpy(val_data['x'])\n",
    "\n",
    "# --- Your code here\n",
    "\n",
    "\n",
    "# ---\n"
   ],
   "metadata": {
    "id": "szjc3ifEpHab"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GRADING GUIDE**\n",
    "\n",
    "Your `MLP` implementation will be graded by evaluating its predictive accuracy on the validation data provided (13 points) as well as a hidden test data set (13 points).\n",
    "Your model will have to have a predicive score of less than 0.01.\n",
    "\n",
    "The remaining 4 points are for the right network architecture."
   ],
   "metadata": {
    "id": "VlvdUiHts3AQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "e4Ag2TTtDON8"
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
